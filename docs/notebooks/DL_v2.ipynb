{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde77563",
   "metadata": {},
   "source": [
    "Author: Paul Inkenbrandt, modified by Diane Menuz for testing  \n",
    "Date: October 1, 2025  \n",
    "Goal: Test modifications to micromet to see about fixing problems, using Escalante as a test case. Clean up  \n",
    "processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda1f66e",
   "metadata": {},
   "source": [
    "Requirements\n",
    "- raw_fold: folder\n",
    "- within raw_fold, subfolders with each stationid\n",
    "- within station-specific folder, the following files:\n",
    "    - {stationname}_Flux_AmeriFluxFormat.dat: Ameriflux file downloaded from EasyFlux; should only be one\n",
    "    - {stationname}_Flux_CSFormat.dat: CSFLUX file downloaded from EasyFlux; should only be one\n",
    "    - folder called AmeriFluxFormat with eddy data downloaded from the station\n",
    "    - folder called Statistics_Ameriflux with met data downloaded from the station\n",
    "    - folder called Statistics with met data downloaded from the station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae8ed42",
   "metadata": {},
   "source": [
    "To Do  \n",
    "\n",
    "- Fix the compare_to_raw component! should generalize the file naming and need to make into a loop if doing all  \n",
    "of the sites?? Only works nonw b/c I am loking at a single site\n",
    "- Could keep report when running through multiple files- I did for the Met Ameriflux Stats and CSFlux, check other   \n",
    "compoments\n",
    "- Move towards a loop for both CSFlux and Ameriflux_format b/c it seems like the downloads are sometimes missing  \n",
    "info; copy the Easyflux data into the same folder as other data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e587dad2",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d356450d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T14:28:18.408562Z",
     "start_time": "2025-09-04T14:28:15.196174Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "sys.path.append(\"../../src/\")\n",
    "import micromet\n",
    "from micromet import validate\n",
    "from micromet import validation\n",
    "from micromet import gap_summary\n",
    "from micromet import cleanup\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb7694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loggerloader_path= \"C:/Users/dmenuz/Documents/scripts/loggerloader\"\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(loggerloader_path)\n",
    "from loggerloader import plotlystuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c349cf54",
   "metadata": {},
   "source": [
    "## Initialize Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f0462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setFormatter(\n",
    "    logging.Formatter(\n",
    "        fmt=\"%(levelname)s [%(asctime)s] %(name)s – %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    ")\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0824f1a",
   "metadata": {},
   "source": [
    "# Define Root Folder and Site Foldes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c940913",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T14:28:19.724869Z",
     "start_time": "2025-09-04T14:28:19.721284Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_fold = pathlib.Path(f'M:/Shared drives/UGS_Flux/Data_Downloads/compiled')\n",
    "\n",
    "#amflux column data\n",
    "amflux = pd.read_csv(r'M:\\Shared drives\\UGS_Flux\\Data_Downloads\\compiled\\flux-met_processing_variables_20250818.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea1625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_folders = {#'US-UTD':'Dugout_Ranch',\n",
    "                # 'US-UTB':'BSF',\n",
    "                 'US-UTJ':'Bluff',\n",
    "                # 'US-UTW':'Wellington',\n",
    "                'US-UTE':'Escalante'\n",
    "                # 'US-UTM':'Matheson',\n",
    "                # 'US-UTP':'Phrag',\n",
    "                # 'US-CdM':'Cedar_mesa',\n",
    "                # 'US-UTV':'Desert_View_Myton',\n",
    "                # 'US-UTN':'Juab',\n",
    "                # 'US-UTG':'Green_River',\n",
    "                # 'US-UTL':'Pelican_Lake',\n",
    "                 }\n",
    "\n",
    "# loggerids = {\n",
    "#     \"eddy\": {\n",
    "#         \"US-UTD\": 21314,\n",
    "#         \"US-UTB\": 27736,\n",
    "#         \"US-UTJ\": 21020,\n",
    "#         \"US-UTW\": 21025,\n",
    "#         \"US-UTE\": 21021,\n",
    "#         \"US-UTM\": 21029,\n",
    "#         \"US-UTP\": 8442,\n",
    "#         \"US-CdM\": 21313,\n",
    "#         \"US-UTV\": 21027,\n",
    "#         \"US-UTN\": 8441,\n",
    "#         \"US-UTG\": 25415,\n",
    "#         \"US-UTL\": 21215,\n",
    "#     },\n",
    "#     \"met\": {\n",
    "#         \"US-UTD\": 21031,\n",
    "#         \"US-UTB\": 27736,\n",
    "#         \"US-UTJ\": 21030,\n",
    "#         \"US-UTW\": 21026,\n",
    "#         \"US-UTE\": 21032,\n",
    "#         \"US-UTM\": 21023,\n",
    "#         \"US-UTP\": 8441,\n",
    "#         \"US-CdM\": 21029,\n",
    "#         \"US-UTV\": 21311,\n",
    "#         \"US-UTG\": 25414,\n",
    "#         \"US-UTL\": 21028,\n",
    "#     },\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb5eef9",
   "metadata": {},
   "source": [
    "# Run Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c4d27a",
   "metadata": {},
   "source": [
    "# Met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7fe141",
   "metadata": {},
   "source": [
    "### Compile Met Statistics Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54752d7c",
   "metadata": {},
   "source": [
    "For each site, loop through all files in that site's Statistics folder, process, and then  \n",
    "compile. Output is a dictionary for each site key with the value as all of the site data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045bef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T13:48:40.593872Z",
     "start_time": "2025-09-04T13:47:53.287851Z"
    }
   },
   "outputs": [],
   "source": [
    "stats = {}\n",
    "reports = {}\n",
    "for key, value in site_folders.items():\n",
    "    print(f\"Processing site: {key} - {value}\")\n",
    "    parent_fold = raw_fold / f\"{key}\" / \"Statistics\"\n",
    "    am_df = {}\n",
    "    stat_report = {}\n",
    "    i=0\n",
    "    #raw_data = micromet.raw_file_compile(raw_fold, parent_fold, search_str = \"TOA5*Statistics*.dat\")    \n",
    "    for file_name in parent_fold.glob(\"TOA5*Statistics*.dat\"):\n",
    "        i += 1\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        sts = pd.read_csv(file_name, skiprows = [0,2,3])\n",
    "        for col in sts.columns:\n",
    "            if col.endswith(\"_Avg\"):\n",
    "                sts.rename(columns={col: col[:-4]}, inplace=True)\n",
    "            elif col.endswith(\"_Tot\"):\n",
    "                sts.rename(columns={col: col[:-4]}, inplace=True)\n",
    "        sts['TIMESTAMP'] = pd.to_datetime(sts['TIMESTAMP'])\n",
    "        sts[\"TIMESTAMP_END\"] = sts.TIMESTAMP.dt.strftime(\"%Y%m%d%H%M\").astype(int)\n",
    "        am_data = micromet.Reformatter(drop_soil=False, logger=logger,)\n",
    "        #raw_data = raw_data.drop([0], axis=0)\n",
    "        df, report, checktime = am_data.process(sts, data_type=\"met\")\n",
    "        am_df[file_name.stem] = df\n",
    "        stat_report[file_name.stem] = report\n",
    "    if i > 0:\n",
    "        stats[key] = pd.concat(am_df)\n",
    "        reports[key] = pd.concat(stat_report)\n",
    "\n",
    "stats_met_temp = pd.concat(stats)\n",
    "stats_reports = pd.concat(reports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220f843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review variables with a lot of dropped values based on the report\n",
    "\n",
    "report_final = stats_reports.reset_index(level=[0,1,2])\n",
    "report_final = report_final.drop(columns=['level_1','level_2'])\n",
    "report_final = report_final.rename({'level_0':'STATIONID'}, axis=1)\n",
    "report_final = report_final.groupby(['STATIONID','column', 'matched_key']).mean()\n",
    "\n",
    "report_final[report_final.pct_flagged>=10].round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7db1024",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_met = stats_met_temp.reset_index().rename(columns={'level_0':'STATIONID'})\n",
    "stats_met = stats_met.drop(['level_1'],axis=1)\n",
    "if len (stats_met[stats_met.duplicated(subset=['STATIONID','DATETIME_END'])])>0:\n",
    "    print('FAIL: STATIONID AND DATETIME_END DUPLICATES PRESENT')\n",
    "    print('DROPPING DUPLICATES')\n",
    "    stats_met = stats_met.drop_duplicates(subset=['STATIONID','DATETIME_END'])\n",
    "\n",
    "else:\n",
    "    print(\"PASS: NO STATIONID AND DATETIME_END DUPLICATES\")\n",
    "\n",
    "stats_met = stats_met.set_index(['STATIONID','DATETIME_END'])\n",
    "stats_met = stats_met.mask(stats_met < -5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdcef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {\n",
    "    \"LWMCON_1\": 'LWMCON_1_1_1',\n",
    "        \"LWMCON_2\": 'LWMCON_1_1_2',\n",
    "        \"LWMDRY_1\": 'LWMDRY_1_1_1',\n",
    "        \"LWMDRY_2\": 'LWMDRY_1_1_2',\n",
    "        \"LWMV_1\": 'LWMV_1_1_1',\n",
    "        \"LWMV_2\": 'LWMV_1_1_2',\n",
    "        \"LWMWET_1\": 'LWMWET_1_1_1',\n",
    "        \"LWMWET_2\": 'LWMWET_1_1_2'\n",
    "    }\n",
    "\n",
    "stats_met.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "results = validate.compare_names_to_ameriflux(stats_met, amflux)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed38c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_met.to_parquet(raw_fold / \"comp_met_stat_diane.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc01e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize and view data gaps (view for just one station)\n",
    "\n",
    "gaps_statsmet = gap_summary.summarize_gaps(stats_met)\n",
    "\n",
    "bal = stats_met.loc['US-UTJ'].sort_index()\n",
    "plotlystuff([bal, bal], ['WS_1_1_2','NETRAD_1_1_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715c9f93",
   "metadata": {},
   "source": [
    "### Compile Statistics Ameriflux .dat Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b64b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldat = {}\n",
    "allreports = {}\n",
    "for key, value in site_folders.items():\n",
    "    print(f\"Processing site: {key} - {value}\")\n",
    "    parent_fold = raw_fold / f\"{key}\" / 'Statistics_Ameriflux'\n",
    "    am_df = {}\n",
    "    report_temp = {}\n",
    "    i=0\n",
    "    #raw_data = micromet.raw_file_compile(raw_fold, parent_fold, search_str = \"TOA5*Statistics*.dat\")    \n",
    "    for file_name in parent_fold.glob(\"*Statistics_AmeriFlux*.dat\"):\n",
    "        print(file_name)\n",
    "        i += 1\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        sts = pd.read_csv(file_name)\n",
    "        for col in sts.columns:\n",
    "            if col.endswith(\"_Avg\"):\n",
    "                sts.rename(columns={col: col[:-4]}, inplace=True)\n",
    "            elif col.endswith(\"_Tot\"):\n",
    "                sts.rename(columns={col: col[:-4]}, inplace=True)\n",
    "        am_data = micromet.Reformatter(drop_soil=False, logger=logger,)\n",
    "        #raw_data = raw_data.drop([0], axis=0)\n",
    "        df, report, checktime = am_data.process(sts, data_type=\"met\")\n",
    "        am_df[file_name.stem] = df\n",
    "        report_temp[file_name.stem] = report\n",
    "    if i > 0:\n",
    "        alldat[key] = pd.concat(am_df)\n",
    "        allreports[key] = pd.concat(report_temp)\n",
    "\n",
    "afstats_met_temp = pd.concat(alldat)\n",
    "outlier_report = pd.concat(allreports, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dce792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review variables with a lot of dropped values based on the report\n",
    "report_stacked = outlier_report.stack(level=0)\n",
    "\n",
    "report_final = report_stacked.reset_index(level=1)\n",
    "report_final = report_final.droplevel(0, axis=0)\n",
    "report_final = report_final.drop(['level_1'], axis=1)\n",
    "report_final.index.name = 'STATIONID'\n",
    "report_final = report_final.reset_index()\n",
    "report_final = report_final.groupby(['STATIONID','column', 'matched_key']).mean()\n",
    "\n",
    "report_final[report_final.pct_flagged>=10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c626834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "afstats_met = afstats_met_temp.reset_index().rename(columns={'level_0':'STATIONID'})\n",
    "afstats_met = afstats_met.drop(['level_1'],axis=1)\n",
    "if len (afstats_met[afstats_met.duplicated(subset=['STATIONID','DATETIME_END'])])>0:\n",
    "    print('FAIL: STATIONID AND DATETIME_END DUPLICATES PRESENT')\n",
    "    print('DROPPING DUPLICATES')\n",
    "    afstats_met = afstats_met.drop_duplicates(subset=['STATIONID','DATETIME_END'])\n",
    "\n",
    "else:\n",
    "    print(\"PASS: NO STATIONID AND DATETIME_END DUPLICATES\")\n",
    "\n",
    "afstats_met = afstats_met.set_index(['STATIONID','DATETIME_END'])\n",
    "afstats_met = afstats_met.mask(afstats_met < -5000)\n",
    "\n",
    "results = cleanup.process_and_match_columns(afstats_met, amflux)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "afstats_met.columns.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51680c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {\n",
    "    \"LWMCON_1\": 'LWMCON_1_1_1',\n",
    "        \"LWMCON_2\": 'LWMCON_1_1_2',\n",
    "        \"LWMDRY_1\": 'LWMDRY_1_1_1',\n",
    "        \"LWMDRY_2\": 'LWMDRY_1_1_2',\n",
    "        \"LWMV_1\": 'LWMV_1_1_1',\n",
    "        \"LWMV_2\": 'LWMV_1_1_2',\n",
    "        \"LWMWET_1\": 'LWMWET_1_1_1',\n",
    "        \"LWMWET_2\": 'LWMWET_1_1_2'\n",
    "    }\n",
    "\n",
    "stats_met.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "results = validate.compare_names_to_ameriflux(stats_met, amflux)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "afstats_met.to_parquet(raw_fold / \"comp_met_afstat_diane.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88befc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaps_afstatsmet = gap_summary.summarize_gaps(afstats_met)\n",
    "\n",
    "bal = afstats_met.loc['US-UTJ'].sort_index()\n",
    "plotlystuff([bal, bal], ['NETRAD_1_1_2','WD_1_1_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfaa4f1",
   "metadata": {},
   "source": [
    "## Check datetimes on available data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be118d75",
   "metadata": {},
   "source": [
    "This is a method to loop through individual met download folders and look at the file dates.  \n",
    "Note that it seems like only the first row of data has a timestamp value, so the end dates are  \n",
    "the same as the start dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701d2ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_list = ['m20240821','m20241008','m20250219','m20250513','m20250715','m20250811','me20241008','met_files']\n",
    "\n",
    "outputs = {}\n",
    "\n",
    "for folder_name in folder_list:\n",
    "    folder = Path(f'M:\\\\Shared drives\\\\UGS_Flux\\\\Data_Downloads\\\\Bluff\\\\{folder_name}')\n",
    "\n",
    "    data_rows = []\n",
    "\n",
    "    for file_name in folder.glob(\"*Statistics_AmeriFlux*.dat\"):\n",
    "        print(f\"Processing: {file_name}\")\n",
    "\n",
    "        try:\n",
    "            sts = pd.read_csv(file_name)\n",
    "            sts['TIMESTAMP_START'] = pd.to_datetime(\n",
    "                 sts['TIMESTAMP_START'],\n",
    "                 format='%Y%m%d%H%M'\n",
    "                 )\n",
    "            row = {\n",
    "                'file_name': file_name.name,  # Use .name to get just the filename string\n",
    "                'min_timestamp': sts.TIMESTAMP_START.min(),\n",
    "                'max_timestamp': sts.TIMESTAMP_START.max(),\n",
    "                'num_records':len(sts)\n",
    "            }\n",
    "            data_rows.append(row)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "            continue  \n",
    "    output_df = pd.DataFrame(data_rows)\n",
    "    outputs[folder_name] = output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40ccd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dates = pd.concat(outputs)\n",
    "mask = (file_dates.min_timestamp>pd.to_datetime('2024-06-01'))\n",
    "file_dates[mask].sort_values(['min_timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40116113",
   "metadata": {},
   "source": [
    "# Eddy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9add59",
   "metadata": {},
   "source": [
    "## Compile Downloaded Eddy Data from EasyFluxWeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209b8f001cc12a9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T14:28:46.991607Z",
     "start_time": "2025-09-04T14:28:33.602504Z"
    }
   },
   "outputs": [],
   "source": [
    "# processing one .dat file per station that is in station folder with \"*_Flux_AmeriFluxFormat.dat\"\n",
    "easyfluxdf = {}\n",
    "ef_reports = {}\n",
    "\n",
    "for key, value in site_folders.items():\n",
    "    site_dir = raw_fold / key\n",
    "    print(site_dir)\n",
    "    for file in site_dir.glob(\"*_Flux_AmeriFluxFormat.dat\"):\n",
    "        print(file)\n",
    "        am_data = micromet.Reformatter(drop_soil=True,\n",
    "                                            logger=logger,\n",
    "                                            )\n",
    "        df = pd.read_csv(file,skiprows=[0,2,3],\n",
    "                        na_values=[-9999,\"NAN\",\"NaN\",\"nan\"])\n",
    "        \n",
    "        am_df, report, checktime = am_data.process(df, data_type=\"eddy\")\n",
    "        easyfluxdf[key] = am_df\n",
    "        ef_reports[key] = report\n",
    "\n",
    "ef_report = pd.concat(ef_reports, axis=1)\n",
    "easyflux = pd.concat(easyfluxdf)\n",
    "\n",
    "ef_report.to_csv(raw_fold / \"easyflux_report_diane.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0204b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review variables with a lot of dropped values based on the report\n",
    "report_stacked = ef_report.stack(level=0)\n",
    "\n",
    "report_final = report_stacked.reset_index(level=1)\n",
    "report_final = report_final.rename(columns={'level_1': 'STATIONID'})\n",
    "\n",
    "report_final[report_final.pct_flagged>=10].sort_values(['STATIONID', 'matched_key']).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b78f401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run various tests on data\n",
    "#raw_file = r'M:\\My Drive\\projects\\eddy_covariance\\site_specific_data_review\\Escalante_Flux_AmeriFluxFormat.dat'\n",
    "\n",
    "easyflux_final = easyflux.reset_index().rename(columns={'level_0':'STATIONID'})\n",
    "\n",
    "validate.validate_flags(easyflux)\n",
    "print('\\n')\n",
    "\n",
    "results = cleanup.process_and_match_columns(easyflux, amflux)\n",
    "print('\\n')\n",
    "\n",
    "validate.validate_timestamp_consistency(easyflux)\n",
    "print('\\n')\n",
    "\n",
    "if len (easyflux_final[easyflux_final.duplicated(subset=['STATIONID','DATETIME_END'])])>0:\n",
    "    print('FAIL: STATIONID AND DATETIME_END DUPLICATES PRESENT')\n",
    "    print('DROPPING DUPLICATES')\n",
    "    easyflux_final = easyflux_final.drop_duplicates(subset=['STATIONID','DATETIME_END'])\n",
    "\n",
    "else:\n",
    "    print(\"PASS: NO STATIONID AND DATETIME_END DUPLICATES\")\n",
    "\n",
    "easyflux_final = easyflux_final.set_index(['STATIONID','DATETIME_END'])\n",
    "easyflux_final = easyflux_final.mask(easyflux_final < -5000)\n",
    "\n",
    "# differences_from_raw = compare_to_raw(raw_file, easyflux, test_var='NETRAD', threshold=0.1)\n",
    "# print('\\n')\n",
    "# print(f'Differences between raw and micromet files')\n",
    "# print(differences_from_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf458ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export parquette file\n",
    "easyflux_final.to_parquet(raw_fold / \"easyflux_diane.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb93635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize and view data gaps (view for just one station)\n",
    "\n",
    "gaps_easyflux = gap_summary.summarize_gaps(easyflux_final)\n",
    "\n",
    "bal = easyflux_final.loc['US-UTJ']\n",
    "plotlystuff([bal, bal, bal], ['LE_1_1_1', 'NETRAD_1_1_1', 'WD_1_1_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df78802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file = r'M:\\Shared drives\\UGS_Flux\\Data_Downloads\\compiled\\US-UTJ\\Bluff_Flux_CSFormat.dat'\n",
    "rawcs = pd.read_csv(raw_file, header=1, skiprows=[2,3],\n",
    "                    na_values=[-9999,\"NAN\",\"NaN\",\"nan\"])\n",
    "\n",
    "format_str = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "rawcs['DATETIME_END'] = pd.to_datetime(\n",
    "    rawcs['TIMESTAMP'].str.strip(), # Always strip just in case!\n",
    "    format=format_str,\n",
    "    errors='coerce'\n",
    ")\n",
    "rawcs = rawcs.set_index(\"TIMESTAMP\").sort_index()\n",
    "rawcs_sub = rawcs[rawcs.LW_OUT <700]\n",
    "\n",
    "\n",
    "plotlystuff([bal, rawcs_sub], ['LW_OUT_1_1_1', 'LW_OUT'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e21a8b0391f9a",
   "metadata": {},
   "source": [
    "## Compile Ameriflux Format dat files from Dataloggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f0c3b9",
   "metadata": {},
   "source": [
    "The other two components just read in a single file per station. This component reads in all of the  \n",
    "datalogger files for all of the stations. Since this can be time-intensive, this is run in two parts.  \n",
    "Part 1 compiles all of the datalogger files into an ouput file in the stations folder. The second part  \n",
    "pulls in all those compiled files, cleans them up, and exports as a parquette file.  \n",
    "  \n",
    "**NOTE:** \n",
    "This method is not preserving the report for each compilation, which is fine. The reports from the other  \n",
    "compilations should be sufficient to provide the general picture of what is going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb0b690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1: Compiling datalogger files together and exporting for each site\n",
    "\n",
    "comp_edd_df = {}\n",
    "outlier_report = {}\n",
    "\n",
    "am = micromet.AmerifluxDataProcessor(logger=logger)\n",
    "\n",
    "for key, value in site_folders.items():\n",
    "\n",
    "    parent_fold = raw_fold / f\"{key}\" / \"AmeriFluxFormat\"\n",
    "    #ahp.scan(parent_fold, min_sim=0.3, backup=False)\n",
    "    #pths = micromet.fix_all_in_parent(parent_fold)\n",
    "    raw_data = am.raw_file_compile(raw_fold, parent_fold, search_str = \"*Flux_AmeriFluxFormat*.dat\")\n",
    "    if raw_data is not None:\n",
    "        am_data = micromet.Reformatter(drop_soil=False,\n",
    "                                       logger=logger,\n",
    "                                       )\n",
    "        #raw_data = raw_data.drop([0], axis=0)\n",
    "        am_df, report, checktime = am_data.process(raw_data, data_type=\"eddy\")\n",
    "        comp_edd_df[key] = am_df\n",
    "        outlier_report[key] = report\n",
    "\n",
    "        timestart = am_df['TIMESTAMP_START'].values[0]\n",
    "        timeend = am_df['TIMESTAMP_END'].values[-1]\n",
    "\n",
    "        am_df.to_csv(raw_fold / f\"{key}\" / f\"{key}_HH_{timestart:}_{timeend:}_diane.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f4c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_edd_df = {}\n",
    "\n",
    "for key, value in site_folders.items():\n",
    "    for file in (raw_fold / f\"{key}\").glob(f\"{key}_HH_*.csv\"):\n",
    "        df = pd.read_csv(file, index_col=0)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df = df.sort_index()\n",
    "        df = df.drop_duplicates(subset=['TIMESTAMP_START','TIMESTAMP_END'])\n",
    "        cmp_edd_df[key] = df\n",
    "\n",
    "datalogger_dat = pd.concat(cmp_edd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run various tests on data and a little cleanup\n",
    "raw_file = r'M:\\Shared drives\\UGS_Flux\\Data_Downloads\\compiled\\US-UTE\\Escalante_Flux_AmeriFluxFormat.dat'\n",
    "\n",
    "dataloggerdf = datalogger_dat.reset_index().rename(columns={'level_0':'STATIONID'})\n",
    "dataloggerdf = dataloggerdf.rename(columns={'BATTERY_VOLTAGE':'V_BATT'})\n",
    "\n",
    "\n",
    "validate.validate_flags(dataloggerdf)\n",
    "print('\\n')\n",
    "\n",
    "results = cleanup.process_and_match_columns(dataloggerdf, amflux)\n",
    "print('\\n')\n",
    "\n",
    "validate.validate_timestamp_consistency(dataloggerdf)\n",
    "print('\\n')\n",
    "\n",
    "# differences_from_raw = validate.compare_to_raw(raw_file, dataloggerdf, test_var='NETRAD', threshold=0.1)\n",
    "# print('\\n')\n",
    "# print(f'Differences between micromet (left) and raw (right) files')\n",
    "# print(differences_from_raw)\n",
    "\n",
    "datalogger_final = dataloggerdf.rename(columns={'BATTERY_VOLTAGE':'V_BATT'}) # tried to fix this with the refromatter_vars but I must have done something wrong...\n",
    "\n",
    "#datalogger_final = datalogger_final.reset_index().rename(columns={'level_0':'STATIONID'})\n",
    "if len (datalogger_final[datalogger_final.duplicated(subset=['STATIONID','DATETIME_END'])])>0:\n",
    "    print('FAIL: STATIONID AND DATETIME_END DUPLICATES PRESENT')\n",
    "    print('DROPPING DUPLICATES')\n",
    "    datalogger_final = datalogger_final.drop_duplicates(subset=['STATIONID','DATETIME_END'])\n",
    "\n",
    "else:\n",
    "    print(\"PASS: NO STATIONID AND DATETIME_END DUPLICATES\")\n",
    "\n",
    "datalogger_final = datalogger_final.set_index(['STATIONID','DATETIME_END'])\n",
    "datalogger_final = datalogger_final.mask(datalogger_final < -5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalogger_final.to_parquet(raw_fold / \"datalogger_diane.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd84f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize and view data gaps (view for just one station)\n",
    "\n",
    "gaps_datalogger = gap_summary.summarize_gaps(datalogger_final)\n",
    "\n",
    "bal = datalogger_final.loc['US-UTE']\n",
    "plotlystuff([bal, bal, bal], ['LE_1_1_1', 'NETRAD_1_1_1', 'WD_1_1_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5299ad4",
   "metadata": {},
   "source": [
    "## Compile CSFormat Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23940cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing one .dat file per station that is in station folder with \"*_Flux_CSFormat.dat\"\n",
    "csdf = {}\n",
    "cs_reports = {}\n",
    "\n",
    "for key, value in site_folders.items():\n",
    "    file_pattern = raw_fold / key / '*_Flux_CSFormat.dat'\n",
    "\n",
    "    try:\n",
    "        file_to_read = next(raw_fold.glob(str(file_pattern.relative_to(raw_fold))))\n",
    "        print(f\"Found file for {key}: {file_to_read.name}\")\n",
    "        df = pd.read_csv(file_to_read,skiprows=[0,2,3],\n",
    "                    na_values=[-9999,\"NAN\",\"NaN\",\"nan\"])\n",
    "        am_data = micromet.Reformatter(drop_soil=True,\n",
    "                                        logger=logger,\n",
    "                                        )\n",
    "        df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])\n",
    "        df[\"TIMESTAMP_END\"] = df.TIMESTAMP.dt.strftime(\"%Y%m%d%H%M\").astype(int)\n",
    "    \n",
    "        csflux_temp, report, timething = am_data.process(df, data_type=\"eddy\")\n",
    "        csdf[key] = csflux_temp\n",
    "        cs_reports[key] = report\n",
    "        \n",
    "    except StopIteration:\n",
    "        # --- File Not Found: Handle the missing file ---\n",
    "        print(f\"⚠️ Warning: No matching *_Flux_CSFormat.dat file found for site: {key} in folder {file_pattern.parent}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "outlier_report = pd.concat(cs_reports, axis=1)\n",
    "cs_dat = pd.concat(csdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53912c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE BELOW SHOULD ALLOW YOU TO ITERATE THROUGH THE FLUX_CSFormat folder to compile all files in there\n",
    "\n",
    "# cs_df = {}\n",
    "# outlier_reports = {}\n",
    "\n",
    "# am = micromet.AmerifluxDataProcessor(logger=logger)\n",
    "\n",
    "# for key, value in site_folders.items():\n",
    "#     sitedf = {}\n",
    "#     sitereport = {}\n",
    "#     parent_fold = raw_fold / f\"{key}\" / \"Flux_CSFormat\"\n",
    "#     #ahp.scan(parent_fold, min_sim=0.3, backup=False)\n",
    "#     #pths = micromet.fix_all_in_parent(parent_fold)\n",
    "#     for file in parent_fold.glob(\"*_Flux_CSFormat*.dat\"):\n",
    "#         am_data = micromet.Reformatter(drop_soil=False,\n",
    "#                                             logger=logger,\n",
    "#                                             )\n",
    "#         df = pd.read_csv(file,skiprows=[0,2,3],\n",
    "#                         na_values=[-9999,\"NAN\",\"NaN\",\"nan\"])\n",
    "#         # must create a timestamp_end column to feed into prepare\n",
    "#         # b/c otherwise no data will be returned\n",
    "#         df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])\n",
    "#         df[\"TIMESTAMP_END\"] = df.TIMESTAMP.dt.strftime(\"%Y%m%d%H%M\").astype(int)\n",
    "        \n",
    "#         csprep, report, checktime = am_data.process(df, data_type=\"eddy\")\n",
    "#         sitedf[file] = csprep\n",
    "#         sitereport[file] = report\n",
    "#     cs_df[key] = pd.concat(sitedf)\n",
    "#     outlier_reports[key] = pd.concat(sitereport)\n",
    "\n",
    "\n",
    "\n",
    "# outlier_report = pd.concat(outlier_reports, axis=1).droplevel(level=0, axis=0)\n",
    "# cs_dat = pd.concat(cs_df).droplevel(level=1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183dbd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review variables with a lot of dropped values based on the report\n",
    "# review variables with a lot of dropped values based on the report\n",
    "report_stacked = outlier_report.stack(level=0)\n",
    "\n",
    "report_final = report_stacked.reset_index(level=1)\n",
    "report_final = report_final.rename(columns = {'level_1':'STATIONID'})\n",
    "report_final = report_final.groupby(['STATIONID','column', 'matched_key']).mean()\n",
    "report_final\n",
    "\n",
    "report_final.to_csv(raw_fold / \"csflux_report_diane.csv\")\n",
    "\n",
    "report_final[report_final.pct_flagged>=10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf23a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_final[(report_final.pct_flagged>=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42a8790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop and/or rename fields we don't want in the file\n",
    "\n",
    "csflux = cs_dat.reset_index().rename(columns={'level_0':'STATIONID'})\n",
    "\n",
    "drop_fields = [\n",
    "    \"TS_CS65X_2_1_1\",\n",
    "    \"WS_RSLT\",\n",
    "    \"_229_DEL_TMPR(1)\",\n",
    "    \"_229_DEL_TMPR(2)\",\n",
    "    \"_229_TMPR_T0_1\",\n",
    "    \"_229_TMPR_T0_2\",\n",
    "    \"_229_TMPR_T1_1\",\n",
    "    \"_229_TMPR_T1_2\",\n",
    "    \"_229_TMPR_T30_1\",\n",
    "    \"_229_TMPR_T30_2\",\n",
    "    \"_PANEL_TMPR_T0\",\n",
    "    \"_PANEL_TMPR_T1\",\n",
    "    \"_PANEL_TMPR_T30\",\n",
    "    \"WND_DIR_STD\",\n",
    "    \"WND_DIR_UNIT_VEC\",\n",
    "    \"WND_SPD_AVG\",\n",
    "    \"U_HEATMAX\",\n",
    "    \"U_SEN0\",\n",
    "    \"U_SENAMP\",\n",
    "    \"U_SENMAX\",\n",
    "    \"SONIC_AZIMUTH\",\n",
    "    \"CS65X_EC_2_1_1\"\n",
    "    \"SUN_AZIMUTH\",\n",
    "    \"SUN_DECLINATION\",\n",
    "    \"SUN_ELEVATION\",\n",
    "    \"HEIGHT_AGL\",\n",
    "    \"HOUR_ANGLE\",\n",
    "    \"CS65X_PERM_1_1_1\",\n",
    "    \"DAYTIME\",\n",
    "    \"E\",\n",
    "    \"E1_Q\",\n",
    "    \"ANONYMOUS1\",\n",
    "    \"ANONYMOUS2\",\n",
    "    \"TD_TP01\",\n",
    "    \"AIR_MASS_COEFF\",\n",
    "    \"ROCP_TP01\",\n",
    "    \"Q\"\n",
    "]\n",
    "\n",
    "for field in drop_fields:\n",
    "    if field in csflux.columns:\n",
    "        csflux = csflux.drop(columns=[field],axis=1)\n",
    "\n",
    "rename_fields = {\n",
    "    \"CS65X_EC_1_1_1\":\"EC_1_1_1\",\n",
    "    \"CS65X_EC_1_1_2\":\"EC_1_1_2\",\n",
    "    \"LI7700_AMB_TMPR\":\"TA_1_1_5\",\n",
    "    \"T_SONIC\":\"T_SONIC_1_1_1\",\n",
    "    'CO2_SIGMA':'CO2_SIGMA_1_1_1', \n",
    "    'H2O_SIGMA':'H2O_SIGMA_1_1_1',\n",
    "    }\n",
    "\n",
    "\n",
    "csflux = csflux.rename(columns=rename_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab7c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run various tests on data; file needs to be downloaded file from easyflux website\n",
    "# skip flags- not in dataframe\n",
    "#raw_file = r'M:\\My Drive\\projects\\eddy_covariance\\site_specific_data_review\\Escalante_Flux_CSFormat.dat'\n",
    "\n",
    "results = cleanup.process_and_match_columns(csflux, amflux)\n",
    "print(results)\n",
    "print('\\n')\n",
    "\n",
    "validate.validate_timestamp_consistency(csflux)\n",
    "print('\\n')\n",
    "\n",
    "# differences_from_raw = validate.compare_to_raw(raw_file, csflux, test_var='NETRAD', threshold=0.1)\n",
    "# print('\\n')\n",
    "# print(f'Differences between micromet (left) and raw (right) files')\n",
    "# print(differences_from_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e967fea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on Paul's merge code; I found that most of the columns didn't exist in my data but I just looked at Escalante\n",
    "# may want to drop the value columns when I am done\n",
    "\n",
    "mergefields = {\n",
    "    \"TA_1_1_4\": [\"AMB_AIR_TMPR\"],\n",
    "    \"E_AMB\": [\"AMB_E\"],\n",
    "    \"E_SAT_AMB\": [\"AMB_E_SAT\"],\n",
    "    \"TS_1_1_1\": [\"TS_CS65X_1_1_1\", \"TS_CS65X_1_1_2\"], \n",
    "    \"TS110_T_AVG\": [\"T_CANOPY\"]\n",
    "}\n",
    "\n",
    "for key, values_list in mergefields.items():\n",
    "    \n",
    "    if key not in csflux.columns:\n",
    "        print(f\"Skipping target '{key}': not found in DataFrame.\")\n",
    "        continue # Skip to the next key\n",
    "    \n",
    "    s_target = csflux[key].replace(-9999, np.nan)\n",
    "    \n",
    "    for value in values_list:\n",
    "        if value in csflux.columns:\n",
    "            print(f\"Merging '{value}' into '{key}'...\")\n",
    "            \n",
    "            s_source = csflux[value].replace(-9999, np.nan)\n",
    "            \n",
    "            s_target = s_target.combine_first(s_source)\n",
    "        else:\n",
    "            print(f\"Source column '{value}' not found, skipping merge into '{key}'.\")\n",
    "            \n",
    "    # Save the final result back to the DataFrame\n",
    "    csflux[key] = s_target.fillna(-9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99711e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for any duplicates and final cleanup\n",
    "duplicate_columns = csflux.columns[csflux.columns.duplicated()]\n",
    "print(\"Duplicate column names:\", duplicate_columns)\n",
    "\n",
    "\n",
    "csflux_final = csflux.reset_index().rename(columns={'level_0':'STATIONID'})\n",
    "if len (csflux_final[csflux_final.duplicated(subset=['STATIONID','DATETIME_END'])])>0:\n",
    "    print('FAIL: STATIONID AND DATETIME_END DUPLICATES PRESENT')\n",
    "    print('DROPPING DUPLICATES')\n",
    "    csflux_final = csflux_final.drop_duplicates(subset=['STATIONID','DATETIME_END'])\n",
    "\n",
    "else:\n",
    "    print(\"PASS: NO STATIONID AND DATETIME_END DUPLICATES\")\n",
    "\n",
    "csflux_final = csflux_final.set_index(['STATIONID','DATETIME_END'])\n",
    "csflux_final = csflux_final.mask(csflux_final < -5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0824f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "csflux_final.to_parquet(raw_fold / \"csflux_diane.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize and view data gaps (view for just one station)\n",
    "\n",
    "gaps_csflux = gap_summary.summarize_gaps(csflux_final)\n",
    "\n",
    "bal = csflux_final.loc['US-UTJ']\n",
    "bal = bal.sort_index()\n",
    "plotlystuff([bal, bal], ['LE_1_1_1', 'NETRAD_1_1_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884a2e27",
   "metadata": {},
   "source": [
    "# Bring together the datasets - NOT REVIEWED!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23696546",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaps_amfluxfmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d667a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_vs_files = gap_summary.compare_gap_summaries(gaps_easyflux, gaps_amfluxfmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8425aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_vs_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ec9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "def fill_missing_from_other(\n",
    "    df_target: pd.DataFrame,\n",
    "    df_source: pd.DataFrame,\n",
    "    expected_freq: str = \"30min\",\n",
    "    add_missing_timestamps: bool = True,\n",
    "    min_steps: int = 1,\n",
    "    columns: list | None = None,\n",
    "    station_level: str = \"STATIONID\",\n",
    "    time_level: str = \"DATETIME_END\",\n",
    "    return_plan: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fill missing values in `df_target` using `df_source` guided by gap/coverage analysis.\n",
    "\n",
    "    It:\n",
    "      1) runs `summarize_gaps` on target and source\n",
    "      2) runs `compare_gap_summaries` to find fillable segments where SOURCE can fill TARGET\n",
    "      3) (optionally) reindexes target to include any missing timestamps in those segments\n",
    "      4) copies values from source -> target ONLY for the targeted column(s), station, and times\n",
    "         where target is missing (NaN or newly added rows)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_target : pd.DataFrame\n",
    "        MultiIndex (station, datetime) with data to be filled (we call this \"A\" internally).\n",
    "    df_source : pd.DataFrame\n",
    "        MultiIndex (station, datetime) with data to copy from (we call this \"B\").\n",
    "    expected_freq : str, default \"30min\"\n",
    "        Grid frequency (must match both datasets).\n",
    "    add_missing_timestamps : bool, default True\n",
    "        If True, adds missing rows in the target during fillable segments before copying.\n",
    "        If False, only fills NaN cells at timestamps that already exist in target.\n",
    "    min_steps : int, default 1\n",
    "        Only consider fillable segments of at least this many samples.\n",
    "    columns : list[str] | None\n",
    "        Optional subset of columns to fill. By default uses the intersection of\n",
    "        df_target.columns and df_source.columns.\n",
    "    station_level : str, default \"STATIONID\"\n",
    "        Name of station level in MultiIndex.\n",
    "    time_level : str, default \"DATETIME_END\"\n",
    "        Name of time level in MultiIndex.\n",
    "    return_plan : bool, default False\n",
    "        If True, also returns the computed fill plan (B→A only).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    filled : pd.DataFrame\n",
    "        A copy of `df_target` with values filled from `df_source`.\n",
    "    audit : pd.DataFrame\n",
    "        Row-by-row audit of realized fills with columns:\n",
    "          ['STATIONID','COLUMN','FILLABLE_START','FILLABLE_END',\n",
    "           'N_STEPS_PLANNED','N_STEPS_FILLED','HOURS_FILLED']\n",
    "    plan (optional) : pd.DataFrame\n",
    "        The B→A portion of the compare plan (only if return_plan=True).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Requires the helper functions `summarize_gaps` and `compare_gap_summaries` to be defined.\n",
    "    - Only copies the specified column indicated by each plan row (no cross-column filling).\n",
    "    - Never overwrites non-missing target values.\n",
    "    \"\"\"\n",
    "    # --- basic checks ---\n",
    "    if not isinstance(df_target.index, pd.MultiIndex) or not isinstance(df_source.index, pd.MultiIndex):\n",
    "        raise TypeError(\"Both df_target and df_source must have a MultiIndex (station, datetime).\")\n",
    "    if station_level not in df_target.index.names or time_level not in df_target.index.names:\n",
    "        raise KeyError(\"df_target index must include levels: station and time.\")\n",
    "    if station_level not in df_source.index.names or time_level not in df_source.index.names:\n",
    "        raise KeyError(\"df_source index must include levels: station and time.\")\n",
    "\n",
    "    # Decide which columns to work on\n",
    "    if columns is None:\n",
    "        columns = list(set(df_target.columns).intersection(set(df_source.columns)))\n",
    "        if not columns:\n",
    "            raise ValueError(\"No overlapping columns between target and source to fill.\")\n",
    "\n",
    "    # Frequency helpers\n",
    "    freq_td = to_offset(expected_freq).delta\n",
    "    hours_per_step = freq_td / pd.Timedelta(hours=1)\n",
    "\n",
    "    # --- Build plan: B fills A ---\n",
    "    gaps_a = gap_summary.summarize_gaps(df_target, station_level=station_level, time_level=time_level,\n",
    "                            expected_freq=expected_freq, columns=columns)\n",
    "    gaps_b = gap_summary.summarize_gaps(df_source, station_level=station_level, time_level=time_level,\n",
    "                            expected_freq=expected_freq, columns=columns)\n",
    "    plan_all = gap_summary.compare_gap_summaries(gaps_a, gaps_b, expected_freq=expected_freq, min_steps=min_steps)\n",
    "    plan = plan_all[plan_all[\"TARGET_DATASET\"] == \"A\"].copy()\n",
    "\n",
    "    if plan.empty:\n",
    "        # Nothing to do\n",
    "        audit = pd.DataFrame(columns=[\n",
    "            \"STATIONID\",\"COLUMN\",\"FILLABLE_START\",\"FILLABLE_END\",\n",
    "            \"N_STEPS_PLANNED\",\"N_STEPS_FILLED\",\"HOURS_FILLED\"\n",
    "        ])\n",
    "        return (df_target.copy(), audit, plan) if return_plan else (df_target.copy(), audit)\n",
    "\n",
    "    # Optional column filter\n",
    "    plan = plan[plan[\"COLUMN\"].isin(columns)].copy()\n",
    "    if plan.empty:\n",
    "        audit = pd.DataFrame(columns=[\n",
    "            \"STATIONID\",\"COLUMN\",\"FILLABLE_START\",\"FILLABLE_END\",\n",
    "            \"N_STEPS_PLANNED\",\"N_STEPS_FILLED\",\"HOURS_FILLED\"\n",
    "        ])\n",
    "        return (df_target.copy(), audit, plan) if return_plan else (df_target.copy(), audit)\n",
    "\n",
    "    # --- Prepare a working copy of target ---\n",
    "    target = df_target.copy()\n",
    "\n",
    "    # If we need to add missing timestamps, compute per-station union of times from plan\n",
    "    if add_missing_timestamps:\n",
    "        add_times_by_station = {}\n",
    "        for _, r in plan.iterrows():\n",
    "            stn = r[\"STATIONID\"]\n",
    "            times = pd.date_range(r[\"FILLABLE_START\"], r[\"FILLABLE_END\"], freq=expected_freq)\n",
    "            add_times_by_station.setdefault(stn, set()).update(times.to_pydatetime().tolist())\n",
    "\n",
    "        # Reindex per station once with the union of needed times\n",
    "        rebuilt = []\n",
    "        stations = target.index.get_level_values(station_level).unique()\n",
    "        stations_in_plan = set(plan[\"STATIONID\"].unique())\n",
    "        for stn in stations.union(stations_in_plan):\n",
    "            # Slice existing station data if present, else empty\n",
    "            if stn in stations:\n",
    "                sub = target.xs(stn, level=station_level)\n",
    "            else:\n",
    "                # Create empty subframe with all columns if station absent\n",
    "                sub = pd.DataFrame(columns=target.columns, index=pd.DatetimeIndex([], name=time_level))\n",
    "\n",
    "            need_times = pd.DatetimeIndex(sorted(add_times_by_station.get(stn, [])))\n",
    "            if len(need_times) > 0:\n",
    "                new_index = sub.index.union(need_times)\n",
    "                sub = sub.reindex(new_index)\n",
    "\n",
    "            # Return to MultiIndex\n",
    "            sub = sub.copy()\n",
    "            sub[station_level] = stn\n",
    "            sub[time_level] = sub.index\n",
    "            sub = sub.set_index([station_level, time_level]).sort_index()\n",
    "            rebuilt.append(sub)\n",
    "\n",
    "        target = pd.concat(rebuilt).sort_index()\n",
    "\n",
    "    # --- Perform the fill per plan row ---\n",
    "    audit_rows = []\n",
    "    idx = pd.IndexSlice\n",
    "    for _, r in plan.iterrows():\n",
    "        stn = r[\"STATIONID\"]\n",
    "        col = r[\"COLUMN\"]\n",
    "        times = pd.date_range(r[\"FILLABLE_START\"], r[\"FILLABLE_END\"], freq=expected_freq)\n",
    "\n",
    "        # Intersect with indices present in both frames (after optional reindex, target has them;\n",
    "        # still be safe if add_missing_timestamps=False)\n",
    "        try:\n",
    "            t_vals = target.loc[idx[stn, times], col]\n",
    "        except KeyError:\n",
    "            # If none of the times exist in target and we didn't reindex them in, skip\n",
    "            continue\n",
    "\n",
    "        # Source values for those times (skip if missing in source for any reason)\n",
    "        try:\n",
    "            s_vals = df_source.loc[idx[stn, times], col]\n",
    "        except KeyError:\n",
    "            # If source lacks all those times (shouldn't happen per plan), skip\n",
    "            continue\n",
    "\n",
    "        # Only fill where target is NA and source is not NA\n",
    "        to_fill_mask = t_vals.isna() & s_vals.notna()\n",
    "        if not to_fill_mask.any():\n",
    "            # Nothing filled for this segment\n",
    "            audit_rows.append({\n",
    "                \"STATIONID\": stn,\n",
    "                \"COLUMN\": col,\n",
    "                \"FILLABLE_START\": r[\"FILLABLE_START\"],\n",
    "                \"FILLABLE_END\": r[\"FILLABLE_END\"],\n",
    "                \"N_STEPS_PLANNED\": int(r[\"N_STEPS_FILLABLE\"]),\n",
    "                \"N_STEPS_FILLED\": 0,\n",
    "                \"HOURS_FILLED\": 0.0,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Assign\n",
    "        fill_index = to_fill_mask.index[to_fill_mask]\n",
    "        target.loc[idx[stn, fill_index], col] = s_vals.loc[fill_index]\n",
    "\n",
    "        n_filled = int(to_fill_mask.sum())\n",
    "        audit_rows.append({\n",
    "            \"STATIONID\": stn,\n",
    "            \"COLUMN\": col,\n",
    "            \"FILLABLE_START\": r[\"FILLABLE_START\"],\n",
    "            \"FILLABLE_END\": r[\"FILLABLE_END\"],\n",
    "            \"N_STEPS_PLANNED\": int(r[\"N_STEPS_FILLABLE\"]),\n",
    "            \"N_STEPS_FILLED\": n_filled,\n",
    "            \"HOURS_FILLED\": n_filled * hours_per_step,\n",
    "        })\n",
    "\n",
    "    audit = pd.DataFrame(audit_rows, columns=[\n",
    "        \"STATIONID\",\"COLUMN\",\"FILLABLE_START\",\"FILLABLE_END\",\n",
    "        \"N_STEPS_PLANNED\",\"N_STEPS_FILLED\",\"HOURS_FILLED\"\n",
    "    ]).sort_values([\"STATIONID\",\"COLUMN\",\"FILLABLE_START\"]).reset_index(drop=True)\n",
    "\n",
    "    # Done\n",
    "    target = target.sort_index()\n",
    "    if return_plan:\n",
    "        return target, audit, plan\n",
    "    return target, audit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0089eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming summarize_gaps() and compare_gap_summaries() are defined (from earlier),\n",
    "# and df_a (target) and df_b (source) are your MultiIndex DataFrames.\n",
    "\n",
    "df_a = pd.read_parquet(raw_fold / \"easyflux.parquet\")\n",
    "df_b = pd.read_parquet(raw_fold / \"comp_edd.parquet\")\n",
    "filled_a, audit = fill_missing_from_other(\n",
    "    df_target=df_a,\n",
    "    df_source=df_b,\n",
    "    expected_freq=\"30min\",\n",
    "    add_missing_timestamps=True,     # add structurally-missing rows before filling\n",
    "    min_steps=1,                     # ignore super-short segments if you want, e.g., min_steps=2\n",
    "    columns=[\"LE_1_1_1\",\"H_1_1_1\",\"NETRAD_1_1_1\",\n",
    "             \"LW_IN_1_1_1\",\"SW_IN_1_1_1\",\"SW_OUT_1_1_1\",\"LW_OUT_1_1_1\"],         # or None to auto-use shared columns\n",
    "    station_level=\"STATIONID\",\n",
    "    time_level=\"DATETIME_END\",\n",
    ")\n",
    "\n",
    "print(audit.head())\n",
    "# filled_a now contains values copied from df_b wherever plan said B could fill A.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15cd822",
   "metadata": {},
   "source": [
    "## Eddy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40345a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edd = pd.read_parquet(raw_fold /  \"comp_edd.parquet\",).replace(-9999,np.nan)\n",
    "df_edd.index.names = ['STATIONID','DATETIME_END']\n",
    "df_edd['PRIORITY'] = 1\n",
    "\n",
    "df = pd.read_parquet(raw_fold /  \"easyflux.parquet\",).replace(-9999,np.nan)\n",
    "df.index.names = ['STATIONID','DATETIME_END']\n",
    "df['PRIORITY'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95c5adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_parquet(raw_fold / \"comp_cs_flux.parquet\")\n",
    "df_merged.index.names = ['STATIONID','DATETIME_END']\n",
    "df_merged['PRIORITY'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bb54aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cff8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdb = pd.read_parquet(raw_fold /  \"old_database_eddy.parquet\",).replace(-9999,np.nan)\n",
    "dfdb.columns = dfdb.columns.str.upper()\n",
    "dfdb['DATETIME_END'] = pd.to_datetime(dfdb['DATETIME_END'])\n",
    "#dfdb[\"TIMESTAMP_START\"] = dfdb['DATETIME_END'].apply(lambda x: f\"{x:%Y%m%d%H%M}\")\n",
    "dfdb = dfdb.set_index(['STATIONID','DATETIME_END'])\n",
    "#df.index.names = ['station','datetime']\n",
    "\n",
    "dfdb.columns = dfdb.columns.str.upper()\n",
    "rename_dict = {'CO2':'CO2_1_1_1', \n",
    "               'CO2_SIGMA':'CO2_SIGMA_1_1_1', \n",
    "               'H2O':'H2O_1_1_1', \n",
    "               'H2O_SIGMA':'H2O_SIGMA_1_1_1',\n",
    "               'FC':'FC_1_1_1', \n",
    "               'FC_SSITC_TEST':'FC_SSITC_TEST_1_1_1', \n",
    "               'LE':'LE_1_1_1',\n",
    "               'LE_SSITC_TEST':'LE_SSITC_TEST_1_1_1', \n",
    "               'ET':'ET_1_1_1',\n",
    "               'ET_SSITC_TEST':'ET_SSITC_TEST_1_1_1', \n",
    "               'H':'H_1_1_1',\n",
    "               'H_SSITC_TEST':'H_SSITC_TEST_1_1_1', \n",
    "               'G':'G_1_1_A',\n",
    "               'G_SSITC_TEST':'G_SSITC_TEST_1_1_1',\n",
    "               'SG':'SG_1_1_1', \n",
    "               'WD':'WD_1_1_1', \n",
    "               'WS':'WS_1_1_1', \n",
    "               'WS_MAX':'WS_MAX_1_1_1',\n",
    "               'PA':'PA_1_1_1', \n",
    "               'VPD':'VPD_1_1_1', \n",
    "               'ALB':'ALB_1_1_1', \n",
    "               'NETRAD':'NETRAD_1_1_1', \n",
    "               'SW_IN':'SW_IN_1_1_1',\n",
    "               'SW_OUT':'SW_OUT_1_1_1', \n",
    "               'LW_IN':'LW_IN_1_1_1', \n",
    "               'LW_OUT':'LW_OUT_1_1_1', \n",
    "               'P':'P_1_1_1', \n",
    "               }\n",
    "\n",
    "dfdb = dfdb.rename(columns=rename_dict)\n",
    "dfdb['ET_1_1_1'].where(dfdb['ET_1_1_1'].between(0,1.1),np.nan)\n",
    "dfdb['PRIORITY'] = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def coalesce_by_priority_multiindex(\n",
    "    df_or_dfs,\n",
    "    priority_col=\"priority\",\n",
    "    ascending=True,\n",
    "    invalid_values=(-9999,),\n",
    "    keep_index=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Column-wise coalesce: for each MultiIndex group (all index levels except `priority_col`),\n",
    "    take the first non-null value per column after sorting by priority.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_or_dfs : DataFrame or list/tuple of DataFrames\n",
    "        Concatenated DataFrame (or list to be concatenated) with a MultiIndex.\n",
    "    priority_col : str\n",
    "        Column name (or index level name) indicating priority. Lower/greater is better\n",
    "        depending on `ascending`.\n",
    "    ascending : bool\n",
    "        Sort so that smaller (True) or larger (False) priority wins.\n",
    "    invalid_values : tuple\n",
    "        Treat these values as missing.\n",
    "    keep_index : bool\n",
    "        Keep the MultiIndex in the result. If False, returns a reset_index frame.\n",
    "    \"\"\"\n",
    "    # 0) Accept list of dfs or a single df\n",
    "    if isinstance(df_or_dfs, (list, tuple)):\n",
    "        df = pd.concat(df_or_dfs, axis=0)\n",
    "    else:\n",
    "        df = df_or_dfs.copy()\n",
    "\n",
    "    # 1) If priority is an index level, move it to a column (so we don't group by it)\n",
    "    if isinstance(df.index, pd.MultiIndex) and priority_col in df.index.names:\n",
    "        df = df.reset_index(level=priority_col)\n",
    "\n",
    "    # 2) Define group levels = all current index levels (MultiIndex) → the \"keys\"\n",
    "    if not isinstance(df.index, pd.MultiIndex):\n",
    "        raise ValueError(\"Expected a MultiIndex index. Set your keys as the DataFrame index first.\")\n",
    "    group_levels = list(df.index.names)\n",
    "\n",
    "    # 3) Value columns = all columns except the priority column\n",
    "    if priority_col not in df.columns:\n",
    "        raise ValueError(f\"'{priority_col}' must be a column or an index level.\")\n",
    "    value_cols = [c for c in df.columns if c != priority_col]\n",
    "\n",
    "    # 4) Treat sentinels as NaN\n",
    "    if invalid_values:\n",
    "        for v in invalid_values:\n",
    "            df[value_cols] = df[value_cols].mask(df[value_cols].eq(v))\n",
    "    df[value_cols] = df[value_cols].where(df[value_cols].notna(), np.nan)\n",
    "\n",
    "    # 5) Sort by priority (best first)\n",
    "    df = df.sort_values(priority_col, ascending=ascending)\n",
    "\n",
    "    # 6) Per group & per column, take the first non-null\n",
    "    def _first_valid(s):\n",
    "        s = s.dropna()\n",
    "        return s.iloc[0] if len(s) else np.nan\n",
    "\n",
    "    out = (\n",
    "        df.groupby(level=group_levels, sort=False)[value_cols]\n",
    "          .agg(_first_valid)\n",
    "    )\n",
    "\n",
    "    return out if keep_index else out.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "result = coalesce_by_priority_multiindex([df,df_edd,df_merged],  \n",
    "                              priority_col=\"PRIORITY\", \n",
    "                              ascending=True, \n",
    "                              invalid_values=(-9999,np.nan,\"NAN\",None))\n",
    "result.to_parquet(raw_fold / \"combined_eddy_dataset_20250905.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d298d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def coalesce_by_priority_multiindex_fast(\n",
    "    df_or_dfs,\n",
    "    priority_col=\"PRIORITY\",\n",
    "    ascending=True,\n",
    "    invalid_values=(-9999,),\n",
    "    keep_index=True,\n",
    "):\n",
    "    # 1) Combine frames\n",
    "    if isinstance(df_or_dfs, (list, tuple)):\n",
    "        df = pd.concat(df_or_dfs, axis=0)\n",
    "    else:\n",
    "        df = df_or_dfs.copy()\n",
    "\n",
    "    # 2) Ensure PRIORITY is an index level (last)\n",
    "    if priority_col in df.columns:\n",
    "        df = df.set_index(priority_col, append=True)\n",
    "    elif not (isinstance(df.index, pd.MultiIndex) and priority_col in df.index.names):\n",
    "        raise ValueError(f\"'{priority_col}' must be a column or an index level.\")\n",
    "    levels = list(df.index.names)\n",
    "    if levels[-1] != priority_col:\n",
    "        levels.remove(priority_col)\n",
    "        levels.append(priority_col)\n",
    "        df = df.reorder_levels(levels).sort_index()\n",
    "\n",
    "    value_cols = list(df.columns)  # all non-index columns\n",
    "\n",
    "    # 3) Normalize invalids → NaN\n",
    "    if invalid_values:\n",
    "        for v in invalid_values:\n",
    "            # Skip np.nan because .eq(np.nan) is always False\n",
    "            if isinstance(v, float) and np.isnan(v):\n",
    "                continue\n",
    "            df[value_cols] = df[value_cols].mask(df[value_cols].eq(v))\n",
    "    df[value_cols] = df[value_cols].where(df[value_cols].notna(), np.nan)\n",
    "\n",
    "    # 4) DEDUP step: collapse duplicates per (keys..., PRIORITY)\n",
    "    #    For each group & column, take the first non-null.\n",
    "    def _first_valid(s):\n",
    "        s = s.dropna()\n",
    "        return s.iloc[0] if len(s) else np.nan\n",
    "\n",
    "    df = (\n",
    "        df.groupby(level=list(df.index.names), sort=False)[value_cols]\n",
    "          .agg(_first_valid)\n",
    "    )\n",
    "\n",
    "    # 5) Unstack PRIORITY and fill across priority dimension (best → worse)\n",
    "    wide = df.unstack(priority_col)  # columns: (value_col, priority)\n",
    "    wide = wide.sort_index(axis=1, level=1, ascending=ascending)\n",
    "    filled = wide.bfill(axis=1)\n",
    "\n",
    "    # 6) Take the first (best) priority slice for each value column\n",
    "    best_priority_label = filled.columns.levels[1][0]\n",
    "    out = filled.xs(best_priority_label, level=1, axis=1)\n",
    "\n",
    "    return out if keep_index else out.reset_index()\n",
    "\n",
    "result2 = coalesce_by_priority_multiindex_fast([df,df_edd,df_merged, dfdb],  \n",
    "                              priority_col=\"PRIORITY\", \n",
    "                              ascending=True, \n",
    "                              invalid_values=(-9999,np.nan,\"NAN\",None))\n",
    "result2.to_parquet(raw_fold / \"combined_eddy_dataset_20250905_v2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037f701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2.loc['US-UTV','NETRAD_1_1_1'].sort_index().plot()\n",
    "plt.ylim(0,800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_static_outliers(\n",
    "    df: pd.DataFrame,\n",
    "    thresh: float = 4.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace values that deviate more than `thresh` standard deviations\n",
    "    from the *station-wide* mean (no moving window).\n",
    "\n",
    "    Outlier detection is performed separately for each station (level-0\n",
    "    of the MultiIndex).  Only floating-point columns are filtered.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        MultiIndex DataFrame with outer index = stationid and inner\n",
    "        index = datetime (half-hourly).\n",
    "    thresh : float, default 3.0\n",
    "        Number of σ from the mean that defines an outlier.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Copy of `df` with outliers in float columns replaced by NaN.\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid mutating the caller’s DataFrame\n",
    "    df = df.copy()\n",
    "\n",
    "    # Select only float columns (ignore integers, objects, etc.)\n",
    "    float_cols = df.select_dtypes(include=[np.floating]).columns\n",
    "    if float_cols.empty:\n",
    "        return df                        # nothing to do\n",
    "\n",
    "    # Compute station-specific mean and std, broadcast back with transform\n",
    "    grp = df[float_cols].groupby(level=0)\n",
    "    mean  = grp.transform(\"mean\")\n",
    "    std   = grp.transform(\"std\")         # sample std (ddof=1) like pandas default\n",
    "\n",
    "    # Identify outliers and replace with NaN\n",
    "    mask = (df[float_cols] - mean).abs() > thresh * std\n",
    "    df.loc[:, float_cols] = df[float_cols].mask(mask)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo = pd.concat([df,df_edd,df_merged],axis=0)\n",
    "# Remove duplicate station datetime values, keeping the non-na values\n",
    "combo = combo.sort_values(['LE_1_1_1','NETRAD_1_1_1','priority']).sort_index()\n",
    "combo = combo.reset_index().drop_duplicates(subset=['stationid','DATETIME_END'],keep='first')\n",
    "combo = combo.set_index(['stationid','DATETIME_END'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't run this- drops most precip values\n",
    "# clean_df = filter_static_outliers(combo, thresh=4)  # custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a16176",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo.to_parquet(raw_fold / \"combined_eddy_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131cc93f",
   "metadata": {},
   "source": [
    "## Met Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f480e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_met = pd.read_parquet(raw_fold /  \"comp_met.parquet\",).replace(-9999,np.nan)\n",
    "df_met.index.names = ['stationid','DATETIME_END']\n",
    "df_met['priority'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c8f2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stmet = pd.read_parquet(raw_fold / \"comp_met_stat.parquet\")\n",
    "stmet['DATETIME_END'] = pd.to_datetime(stmet['TIMESTAMP_START'],format=\"%Y%m%d%H%M\")\n",
    "stmet = stmet.reset_index()\n",
    "stmet = stmet.rename(columns = {'level_0':'stationid'})\n",
    "stmet = stmet.set_index(['stationid','DATETIME_END'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca9091",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdbm = pd.read_parquet(raw_fold /  \"old_database_met.parquet\",).replace(-9999,np.nan)\n",
    "dfdbm['DATETIME_END'] = pd.to_datetime(dfdbm['DATETIME_END'])\n",
    "dfdbm = dfdbm.set_index(['stationid','DATETIME_END'])\n",
    "#df.index.names = ['station','datetime']\n",
    "\n",
    "dfdbm.columns = dfdbm.columns.str.upper()\n",
    "rename_dict_m = {'CO2':'CO2_1_1_2', \n",
    "               'CO2_SIGMA':'CO2_SIGMA_1_1_2', \n",
    "               'H2O':'H2O_1_1_2', \n",
    "               'H2O_SIGMA':'H2O_SIGMA_1_1_2',\n",
    "               'FC':'FC_1_1_2', \n",
    "               'FC_SSITC_TEST':'FC_SSITC_TEST_1_1_2', \n",
    "               'LE':'LE_1_1_2',\n",
    "               'LE_SSITC_TEST':'LE_SSITC_TEST_1_1_2', \n",
    "               'ET':'ET_1_1_2',\n",
    "               'ET_SSITC_TEST':'ET_SSITC_TEST_1_1_2', \n",
    "               'H':'H_1_1_2',\n",
    "               'H_SSITC_TEST':'H_SSITC_TEST_1_1_2', \n",
    "               'G':'G_1_1_A',\n",
    "               'G_SSITC_TEST':'G_SSITC_TEST_1_1_2',\n",
    "               'SG':'SG_1_1_2', \n",
    "               'WD':'WD_1_1_2', \n",
    "               'WS':'WS_1_1_2', \n",
    "               'WS_MAX':'WS_MAX_1_1_2',\n",
    "               'PA':'PA_1_1_2', \n",
    "               'VPD':'VPD_1_1_2', \n",
    "               'ALB':'ALB_1_1_2', \n",
    "               'NETRAD':'NETRAD_1_1_2', \n",
    "               'SW_IN':'SW_IN_1_1_2',\n",
    "               'SW_OUT':'SW_OUT_1_1_2', \n",
    "               'LW_IN':'LW_IN_1_1_2', \n",
    "               'LW_OUT':'LW_OUT_1_1_2', \n",
    "               'P':'P_1_1_2', \n",
    "               }\n",
    "\n",
    "dfdbm = dfdbm.rename(columns=rename_dict_m)\n",
    "#dfdb['ET_1_1_1'].where(dfdb['ET_1_1_1'].between(0,1.1),np.nan)\n",
    "dfdbm['priority'] = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eed569",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_met = pd.concat([df_met,dfdbm,stmet],axis=0)\n",
    "combo_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328650af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate station datetime values, keeping the non-na values\n",
    "combo_met = combo_met.sort_values(['NETRAD_1_1_2','priority']).sort_index()\n",
    "combo_met = combo_met.reset_index().drop_duplicates(subset=['stationid','DATETIME_END'],keep='first')\n",
    "combo_met = combo_met.set_index(['stationid','DATETIME_END'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c7fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# may want to revisit whether to run this- caused issues with precip data for the eddy stations\n",
    "# clean_df_met = filter_static_outliers(combo_met, thresh=4)  # custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4556bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df_met.to_parquet(raw_fold / \"combined_met_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d5549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57c802",
   "metadata": {},
   "outputs": [],
   "source": [
    "met  = pd.read_parquet(raw_fold / \"combined_met_dataset.parquet\")\n",
    "eddy = pd.read_parquet(raw_fold / \"combined_eddy_dataset.parquet\")\n",
    "#met.to_csv(raw_fold / \"combined_met_dataset.csv\")\n",
    "#eddy.to_csv(raw_fold / \"combined_eddy_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281585a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.merge(met, eddy, how='outer', left_index=True, right_index=True,\n",
    "         suffixes=('_met', '_eddy'))\n",
    "\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c49ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.loc['US-UTD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d887f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.loc['US-UTD', ['WS','WS_1_1_1']].dropna().plot(kind='scatter',x='WS',y='WS_1_1_1',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fb9279",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_cols = [\"WS\", \n",
    "                \"TA_\", \n",
    "                \"RH_\", \n",
    "                \"LE_\", \n",
    "                \"H_\", \n",
    "                \"VPD\", \n",
    "                \"PA\", \n",
    "                \"WD\", \n",
    "                \"NETRAD\", \n",
    "                \"SW_IN_\", \n",
    "                \"SW_OUT_\", \n",
    "                \"LW_IN_\", \n",
    "                \"LW_OUT_\", \n",
    "                \"ALB\"]\n",
    "\n",
    "matches = {}\n",
    "for i in compare_cols:\n",
    "    values = []\n",
    "    met_col = []\n",
    "    eddy_col = []\n",
    "\n",
    "    for col in met.columns:\n",
    "        if 'MAX' not in col and 'SSITC' not in col:\n",
    "            if col.startswith(i):\n",
    "                values.append(col)\n",
    "                met_col.append(col)\n",
    "\n",
    "    for col in eddy.columns:\n",
    "        if 'MAX' not in col and 'SSITC' not in col:\n",
    "            if col.startswith(i):\n",
    "                values.append(col)\n",
    "                eddy_col.append(col)\n",
    "\n",
    "    matches[i] = values\n",
    "    if len(values) > 1:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        plt.title(f\"Comparison of {i} for US-UTD\")\n",
    "        for j in met_col:\n",
    "            met.loc['US-UTD',j].replace(-9999,np.nan).plot(label=j,ax=ax)\n",
    "        for k in eddy_col:\n",
    "            eddy.loc['US-UTD',k].replace(-9999,np.nan).plot(label=k,ax=ax)\n",
    "        plt.legend()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccca283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from collections import defaultdict\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. LOAD  (needs pyarrow or fastparquet installed)\n",
    "# --------------------------------------------------\n",
    "met  = pd.read_parquet(raw_fold / \"combined_met_dataset.parquet\")\n",
    "eddy = pd.read_parquet(raw_fold / \"combined_eddy_dataset.parquet\")\n",
    "\n",
    "# If not already multi-indexed by (station, timestamp):\n",
    "# met  = met.set_index([\"station_id\", \"timestamp\"]).sort_index()\n",
    "# eddy = eddy.set_index([\"station_id\", \"timestamp\"]).sort_index()\n",
    "\n",
    "# Keep only overlapping station–time rows\n",
    "common_idx = met.index.intersection(eddy.index)\n",
    "met, eddy  = met.loc[common_idx], eddy.loc[common_idx]\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. DEFINE THE PREFIXES YOU WANT TO COMPARE\n",
    "#    (fill this list in with your own)\n",
    "# --------------------------------------------------\n",
    "prefixes = [\"WS\", \"TA\", \"RH\", \"LE\", \"H\", \"VPD\", \"PA\", \"WD\", \"NETRAD\", \"SW_IN\", \"SW_OUT\", \"LW_IN\", \"LW_OUT\", \"ALB\"]\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. BUILD A MATCH TABLE  {prefix -> [(met_col, eddy_col), …]}\n",
    "# --------------------------------------------------\n",
    "matches = defaultdict(list)\n",
    "\n",
    "for p in prefixes:\n",
    "    # columns that begin with that prefix\n",
    "    met_cols  = [c for c in met.columns  if c.startswith(p)]\n",
    "    eddy_cols = [c for c in eddy.columns if c.startswith(p)]\n",
    "\n",
    "    # simplest strategy: look for *exact* column-name matches\n",
    "    common = set(met_cols).intersection(eddy_cols)\n",
    "    for col in common:\n",
    "        matches[p].append((col, col))\n",
    "\n",
    "    # fallback: if names differ after the prefix, pair by the suffix\n",
    "    if not common:\n",
    "        met_suffix  = {c[len(p):]: c for c in met_cols}\n",
    "        eddy_suffix = {c[len(p):]: c for c in eddy_cols}\n",
    "        for suf in met_suffix.keys() & eddy_suffix.keys():\n",
    "            matches[p].append((met_suffix[suf], eddy_suffix[suf]))\n",
    "\n",
    "# sanity check\n",
    "if not any(matches.values()):\n",
    "    raise ValueError(\"No columns matched with the given prefixes!\")\n",
    "else:\n",
    "    print(f\"Found {len(matches)} prefixes with matches:\")\n",
    "    for p, pairs in matches.items():\n",
    "        print(f\"  {p}: {len(pairs)} pairs\")\n",
    "        for mcol, ecol in pairs:\n",
    "            print(f\"    {mcol} ↔ {ecol}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. COLLECT ALL DIFFERENCES INTO ONE DATAFRAME\n",
    "#    (column names => \"<prefix><suffix>_diff\")\n",
    "# --------------------------------------------------\n",
    "diff_frames = []\n",
    "for p, pairs in matches.items():\n",
    "    for mcol, ecol in pairs:\n",
    "        name = f\"{mcol}_diff\"          # keeps original met name for clarity\n",
    "        diff_frames.append(\n",
    "            (name, met[mcol] - eddy[ecol])\n",
    "        )\n",
    "\n",
    "# combine into a single MultiIndex-friendly DataFrame\n",
    "diff = pd.concat(\n",
    "    {name: series for name, series in diff_frames}, axis=1\n",
    ")\n",
    "\n",
    "abs_diff = diff.abs()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. OUTLIER METHODS\n",
    "# --------------------------------------------------\n",
    "# 5A. Z-score (3σ)\n",
    "z_scores = abs_diff.groupby(level=0).transform(\n",
    "    lambda g: (g - g.mean()) / g.std(ddof=0)\n",
    ")\n",
    "flags_z = z_scores > 3\n",
    "\n",
    "# 5B. MAD (3.5× MAD)\n",
    "def mad_flags(s, k=3.5):\n",
    "    med = s.median()\n",
    "    mad = np.median(np.abs(s - med))\n",
    "    return np.abs(s - med) / (1.4826 * mad + 1e-9) > k\n",
    "\n",
    "flags_mad = abs_diff.groupby(level=0).transform(mad_flags)\n",
    "\n",
    "# 5C. Isolation Forest (multivariate, per station)\n",
    "flags_if = pd.DataFrame(False, index=abs_diff.index, columns=abs_diff.columns)\n",
    "\n",
    "for stn, g in abs_diff.groupby(level=0):\n",
    "    X   = g.values\n",
    "    ok  = np.any(~np.isnan(X), axis=1)\n",
    "    if ok.sum() < 20:                # need enough rows to fit\n",
    "        continue\n",
    "\n",
    "    clf = IsolationForest(\n",
    "        n_estimators=300,\n",
    "        contamination=0.01,\n",
    "        random_state=42,\n",
    "    ).fit(X[ok])\n",
    "\n",
    "    row_out = clf.predict(X[ok]) == -1   # → Boolean vector\n",
    "    # broadcast to all columns\n",
    "    flags_if.loc[g.index[ok], :] = np.repeat(\n",
    "        row_out[:, None], g.shape[1], axis=1\n",
    "    )\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6. QUICK SUMMARY  (how many flags per variable)\n",
    "# --------------------------------------------------\n",
    "summary = (\n",
    "    pd.DataFrame({\n",
    "        \"Zscore\": flags_z.sum(),\n",
    "        \"MAD\":    flags_mad.sum(),\n",
    "        \"IsoF\":   flags_if.sum(),\n",
    "    })\n",
    "    .sort_index()\n",
    ")\n",
    "print(summary.head())\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 7. OPTIONAL:  EXPORT OR APPLY MASK\n",
    "# --------------------------------------------------\n",
    "# Example: mask out any value flagged by *any* method\n",
    "combined_flags = flags_z | flags_mad | flags_if\n",
    "clean_met  = met.where(~combined_flags)  # replaces flagged cells with NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29359bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "met_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adfd5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# ---------- 1. LOAD ----------\n",
    "met   = pd.read_parquet(raw_fold /\"combined_met_dataset.parquet\")   # needs pyarrow or fastparquet\n",
    "eddy  = pd.read_parquet(raw_fold /\"combined_eddy_dataset.parquet\")\n",
    "\n",
    "# If your indices aren’t yet a MultiIndex (station, time) do this once:\n",
    "# met  = met.set_index([\"station_id\",\"timestamp\"]).sort_index()\n",
    "# eddy = eddy.set_index([\"station_id\",\"timestamp\"]).sort_index()\n",
    "\n",
    "# Keep only the overlapping stations & times\n",
    "common_idx = met.index.intersection(eddy.index)\n",
    "met  = met.loc[common_idx]\n",
    "eddy = eddy.loc[common_idx]\n",
    "\n",
    "# ---------- 2. IDENTIFY MATCHING VARIABLES ----------\n",
    "common_cols = met.columns.intersection(eddy.columns)\n",
    "if common_cols.empty:\n",
    "    raise ValueError(\"No shared measurement names between the two datasets!\")\n",
    "\n",
    "# Optionally drop columns that are integer-typed (often flags / counters)\n",
    "keep_float = [c for c in common_cols if np.issubdtype(met[c].dtype, np.floating)]\n",
    "met  = met[keep_float]\n",
    "eddy = eddy[keep_float]\n",
    "\n",
    "# ---------- 3. STACK THE TWO SOURCES FOR EZ COMPARISON ----------\n",
    "diff = met - eddy              # sign tells you which source is higher\n",
    "abs_diff = diff.abs()\n",
    "\n",
    "# ---------- 4A. Z-SCORE BASED OUTLIERS ----------\n",
    "z_scores = abs_diff.groupby(level=0).transform(  # compute σ station-by-station\n",
    "    lambda g: (g - g.mean()) / g.std(ddof=0)\n",
    ")\n",
    "outliers_z = z_scores > 3        # boolean DF same shape as diff\n",
    "\n",
    "# ---------- 4B. MAD BASED OUTLIERS ----------\n",
    "def mad_based_flags(series, k=3.5):\n",
    "    med = series.median()\n",
    "    mad = np.median(np.abs(series - med))\n",
    "    # 1.4826 converts MAD to σ for a normal dist.\n",
    "    return np.abs(series - med) / (1.4826 * mad + 1e-9) > k\n",
    "\n",
    "outliers_mad = abs_diff.groupby(level=0).transform(mad_based_flags)\n",
    "\n",
    "# ---------- 4C. ISOLATION FOREST (multivariate) ----------\n",
    "iso_out = {}\n",
    "for stn, g in abs_diff.groupby(level=0):\n",
    "\n",
    "    X = g.values\n",
    "    mask = np.any(~np.isnan(X), axis=1)          # rows with ≥1 real number\n",
    "    flags = pd.DataFrame(False, index=g.index, columns=g.columns)\n",
    "\n",
    "    if mask.sum() >= 20:                         # enough samples to train\n",
    "        clf = IsolationForest(\n",
    "            contamination=0.01,\n",
    "            n_estimators=300,\n",
    "            random_state=42,\n",
    "        ).fit(X[mask])\n",
    "\n",
    "        row_flags = clf.predict(X[mask]) == -1   # 1-D Boolean (outlier rows)\n",
    "\n",
    "        # --- broadcast row_flags to full (n_rows_selected × n_columns) matrix\n",
    "        flags.iloc[mask, :] = np.repeat(\n",
    "            row_flags[:, None], g.shape[1], axis=1\n",
    "        )\n",
    "\n",
    "    iso_out[stn] = flags\n",
    "\n",
    "outliers_iso = pd.concat(iso_out)\n",
    "\n",
    "# ---------- 5. SUMMARIZE ----------\n",
    "summary = (\n",
    "    pd.DataFrame({\n",
    "        \"z_score\":  outliers_z.sum(),\n",
    "        \"MAD\":      outliers_mad.sum(),\n",
    "        \"iForest\":  outliers_iso.sum()\n",
    "    })\n",
    "    .rename_axis(\"variable\")\n",
    ")\n",
    "print(summary.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8792cc9c",
   "metadata": {},
   "source": [
    "Compile files from each station into a a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d61bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.concat(comp_edd_df, axis=0)\n",
    "cdf.index.set_names(['stationid','DATETIME_END'],inplace=True)\n",
    "#cdf.rename(columns={'level_0':'stationid'},inplace=True)\n",
    "#cdf.to_parquet('../station_data/all_data.parquet')\n",
    "for col in cdf.columns:\n",
    "    cdf.rename(columns={col:col.lower()},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf106f08",
   "metadata": {},
   "source": [
    "Save to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d86dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.to_parquet('../../station_data/all_eddy_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f62bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577962db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comp_met_df = {}\n",
    "root_dir = \"C:/Users/paulinkenbrandt/Documents/GitHub/MicroMet/src/micromet/data/\"\n",
    "config_path = root_dir + \"reformatter_vars.yml\"\n",
    "var_limits_csv = root_dir + \"extreme_values.csv\"\n",
    "am = micromet.AmerifluxDataProcessor(config_path, logger)\n",
    "\n",
    "\n",
    "for key, value in site_folders.items():\n",
    "\n",
    "    print(key)\n",
    "    raw_fold = pathlib.Path('G:/Shared drives/UGS_Flux/Data_Downloads/')\n",
    "    raw_data = am.raw_file_compile(raw_fold, value, search_str = \"*Statistics_AmeriFlux*.dat\")\n",
    "    if raw_data is not None:\n",
    "        am_data = micromet.Reformatter(\n",
    "                                       config_path=config_path,\n",
    "                                       var_limits_csv= var_limits_csv,\n",
    "                                       drop_soil=False,\n",
    "                                       logger=logger,\n",
    "                                       )\n",
    "        am_df = am_data.process(raw_data, data_type=\"met\")\n",
    "        #am_df = am_data.et_data\n",
    "        comp_met_df[key] = am_df\n",
    "\n",
    "        #am_df.to_csv(f\"../../station_data/{key}_HH_{am_df['TIMESTAMP_START'].values[0]:}_{am_df['TIMESTAMP_END'].values[-1]:}.csv\")\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64217e3c28f06cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.columns = ddf.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d44a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "soildfs\n",
    "\n",
    "for old_col, new_col in mapping.items():\n",
    "    if str(old_col).lower() in soildfs.columns.str.lower():\n",
    "        if str(new_col).lower() in soildfs.columns.str.lower():\n",
    "            soildfs[new_col.lower()] = soildfs[[old_col.lower(), new_col.lower()]].max(axis=1)\n",
    "            soildfs = soildfs.drop(old_col.lower(), axis=1)\n",
    "        else:\n",
    "            soildfs = soildfs.rename(columns={old_col.lower(): new_col.lower()})\n",
    "    elif str(old_col).lower()+\"_eddy\" in soildfs.columns.str.lower():\n",
    "        print(f\"Found {old_col} eddy column\")\n",
    "        if str(new_col).lower()+\"_eddy\" in soildfs.columns.str.lower():\n",
    "            soildfs[new_col.lower()] = soildfs[[old_col.lower()+\"_eddy\", new_col.lower()+\"_eddy\"]].max(axis=1)\n",
    "            soildfs = soildfs.drop(old_col.lower()+\"_eddy\", axis=1)\n",
    "        else:\n",
    "            soildfs = soildfs.rename(columns={old_col.lower()+\"_eddy\": new_col.lower()})\n",
    "    elif str(new_col).lower()+\"_eddy\" in soildfs.columns.str.lower():\n",
    "        if str(new_col).lower() in soildfs.columns.str.lower():\n",
    "            soildfs[new_col.lower()] = soildfs[[new_col.lower()+\"_eddy\", new_col.lower()+\"_eddy\"]].max(axis=1)\n",
    "            soildfs = soildfs.drop(new_col.lower()+\"_eddy\", axis=1)\n",
    "            print(f\"Found {new_col} eddy column\")\n",
    "        else:\n",
    "            print(f\"Found {new_col} eddy column\")\n",
    "            soildfs = soildfs.rename(columns={new_col.lower()+\"_eddy\": new_col.lower()})\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.concat(comp_met_df, axis=0)\n",
    "ddf.index.set_names(['stationid','DATETIME_END'],inplace=True)\n",
    "#cdf.rename(columns={'level_0':'stationid'},inplace=True)\n",
    "#cdf.to_parquet('../station_data/all_data.parquet')\n",
    "for col in ddf.columns:\n",
    "    ddf.rename(columns={col:col.lower()},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf042fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[~ddf['vwc_2_7_1'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef3316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.iloc[0:1,:].to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f082e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "soilcols = [col.lower() for col in am_data.MATH_SOILS_V2]\n",
    "pattern = re.compile(r\"2_1_1|1_2_1|1_1_2\")\n",
    "# Print matching columns\n",
    "matching_cols = [col for col in soilcols if pattern.search(col)]\n",
    "# Remove them from the original list\n",
    "soilcols = [col for col in soilcols if not pattern.search(col)]\n",
    "\n",
    "        \n",
    "soildfs = pd.merge(ddf,cdf[soilcols],how='left',on=['stationid','DATETIME_END'],suffixes=(None,'_eddy'))\n",
    "soildfs\n",
    "\n",
    "for col in cdf.columns:\n",
    "    if col in soilcols:\n",
    "        cdf.drop(columns=col,inplace=True)  # drop the soil columns from the main dataframe\n",
    "\n",
    "cdf.to_parquet('../../station_data/all_eddy_data.parquet')\n",
    "\n",
    "soildfs.to_parquet('../../station_data/all_soil_data.parquet')\n",
    "\n",
    "ddf.to_parquet('../../station_data/all_met_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f9e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.read_parquet('../../station_data/all_eddy_data.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84295da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cc9ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "soildfs = pd.read_parquet('../../station_data/all_soil_data.parquet')\n",
    "utd_soilt = soildfs.loc['US-UTD'][['ts_3_1_1','ts_3_2_1','ts_3_3_1']].replace(-9999,np.nan)\n",
    "utd_soilt = utd_soilt[utd_soilt.index >= '2024-07-01']#.resample('30T').mean()\n",
    "utd_soilt['ts_3_1_1'].plot()\n",
    "utd_soilt['ts_3_2_1'].shift(-1).plot()\n",
    "utd_soilt['ts_3_3_1'].shift(-5).plot()\n",
    "plt.axvline('2024-07-04 15:00',color='r')\n",
    "#plt.xlim('2024-07-01','2024-07-08')\n",
    "#plt.ylim(10,35)\n",
    "plt.grid(True, which='minor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy.signal import correlate\n",
    "\n",
    "# Function to decompose the seasonal component\n",
    "def extract_seasonal(ts, period):\n",
    "    decomposition = seasonal_decompose(ts, model='additive', period=period)\n",
    "    return decomposition.seasonal\n",
    "\n",
    "# Function to calculate lag between two seasonal series using cross-correlation\n",
    "def calculate_lag(seasonal1, seasonal2):\n",
    "    n = len(seasonal1)\n",
    "    correlation = correlate(seasonal1 - np.mean(seasonal1), seasonal2 - np.mean(seasonal2), mode='full')\n",
    "    lags = np.arange(-n + 1, n)\n",
    "    lag = lags[np.argmax(correlation)]\n",
    "    return lag, correlation, lags\n",
    "\n",
    "ts1 = utd_soilt['ts_3_2_1']\n",
    "ts2 = utd_soilt['ts_3_3_1']\n",
    "#utd_soilt['ts_3_3_1'].shift(-5).plot()\n",
    "\n",
    "\n",
    "# Extract seasonal components\n",
    "seasonal1 = extract_seasonal(ts1, period=48)\n",
    "seasonal2 = extract_seasonal(ts2, period=48)\n",
    "\n",
    "# Calculate lag\n",
    "lag, correlation, lags = calculate_lag(seasonal1.dropna(), seasonal2.dropna())\n",
    "\n",
    "# Output\n",
    "print(f\"Calculated lag: {lag/2} hours\")\n",
    "\n",
    "# Plot seasonal components and correlation\n",
    "fig, ax = plt.subplots(3, 1, figsize=(10, 8))\n",
    "\n",
    "seasonal1.plot(ax=ax[0], label='Seasonal Component 1')\n",
    "seasonal2.plot(ax=ax[0], label='Seasonal Component 2')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Seasonal Components')\n",
    "ax[0].set_xlim(pd.to_datetime('2024-07-01'),pd.to_datetime('2024-07-08'))\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(lags, correlation)\n",
    "ax[1].set_title('Cross-Correlation')\n",
    "ax[1].set_xlabel('Lag (hours)')\n",
    "ax[1].set_ylabel('Correlation')\n",
    "ax[1].set_xlim(-10, 10)\n",
    "ax[1].grid(True)\n",
    "\n",
    "ax[2].plot(seasonal1.index, seasonal1, label='Series 1')\n",
    "ax[2].plot(seasonal2.index + pd.Timedelta(hours=lag/2), seasonal2, label='Series 2 (Shifted)')\n",
    "ax[2].legend()\n",
    "ax[2].set_title(f'Series alignment (Lag: {lag/2} hours)')\n",
    "ax[2].set_xlim(pd.to_datetime('2024-07-01'),pd.to_datetime('2024-07-08'))\n",
    "ax[2].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04ee994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc536c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.read_parquet('../../station_data/all_eddy_data.parquet')\n",
    "ddf = pd.read_parquet('../../station_data/all_met_data.parquet')\n",
    "\n",
    "for col in cdf.columns:\n",
    "    if col in ddf.columns:\n",
    "        print(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head(10).to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549094d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = ddf.loc['US-UTD','t_si111_body'].replace(-9999,np.nan)\n",
    "series.plot()\n",
    "series.diff().plot()\n",
    "new_series = series[series.diff()<2].diff().cumsum()\n",
    "new_series.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddcebdfd6a7b51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "config.read('../../secrets/config.ini')\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import urllib.parse\n",
    "host = config['DEFAULT']['ip']\n",
    "pw = config['DEFAULT']['pw']\n",
    "user = config['DEFAULT']['login']\n",
    "\n",
    "encoded_password = urllib.parse.quote_plus(pw)\n",
    "\n",
    "def postconn_et(encoded_password, host='localhost',user='postgres',port='5432',db='groundwater', schema = 'groundwater'):\n",
    "    connection_text = \"postgresql+psycopg2://{:}:{:}@{:}:{:}/{:}?gssencmode=disable\".format(user,encoded_password,host,port,db)\n",
    "    return create_engine(connection_text, connect_args={'options': '-csearch_path={}'.format(schema)})\n",
    "\n",
    "\n",
    "engine = postconn_et(encoded_password, host=host, user=user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ce788442a9680",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.to_sql(name = 'amfluxeddy',\n",
    "           schema='groundwater',\n",
    "           con=engine,\n",
    "           if_exists='replace',\n",
    "           chunksize=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f92cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in soildfs.columns:\n",
    "    print(f\"amfluxmet.{col},\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5819ddd94230e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "soildfs.to_sql(name = 'amfluxmet',\n",
    "           schema='groundwater',\n",
    "           con=engine,\n",
    "           if_exists='replace',\n",
    "           chunksize=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygis12v3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
