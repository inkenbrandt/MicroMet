{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6cb5d9c8ae57b4",
   "metadata": {},
   "source": [
    "This notebook was used to compile all of the available data from the Utah Flux Network stations.  It should only need to be used once, as other notebooks are used to comile the newer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "import pathlib\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.parse import quote\n",
    "from sqlalchemy import create_engine\n",
    "import configparser\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "#import pingouin as pg\n",
    "import plotly.express as px\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e3a29165852b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ys.path.append(\"//\")\n",
    "#ys.path.append(\"//\")\n",
    "sys.path.append(\"../../../Micromet\")\n",
    "import micromet\n",
    "from micromet import AmerifluxDataProcessor\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad20f401f01ce92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_folders = {'US-UTD':'Dugout_Ranch',\n",
    "                'US-UTB':'BSF',\n",
    "                'US-UTJ':'Bluff',\n",
    "                'US-UTW':'Wellington',\n",
    "                'US-UTE':'Escalante',\n",
    "                'US-UTM':'Matheson',\n",
    "                'US-UTP':'Phrag',\n",
    "                'US-CdM':'Cedar_mesa',\n",
    "                'US-UTV':'Desert_View_Myton',\n",
    "                'US-UTN':'Juab'\n",
    "                }\n",
    "\n",
    "\n",
    "compdf = {}\n",
    "\n",
    "am = micromet.AmerifluxDataProcessor()\n",
    "\n",
    "for key, value in site_folders.items():\n",
    "\n",
    "    print(key)\n",
    "    raw_fold = pathlib.Path('G:/Shared drives/UGS_Flux/Data_Downloads/')\n",
    "    raw_data = am.raw_file_compile(raw_fold, value, search_str = \"*Flux_AmeriFluxFormat*.dat\")\n",
    "    if raw_data is not None:\n",
    "        am_data = micromet.Reformatter(raw_data,\n",
    "                                       config_path=\"../../data/reformatter_vars.yml\", \n",
    "                                       drop_soil=False,)\n",
    "        am_df = am_data.et_data\n",
    "        compdf[key] = am_df\n",
    "\n",
    "        am_df.to_csv(f\"../../station_data/{key}_HH_{am_df['TIMESTAMP_START'].values[0]:}_{am_df['TIMESTAMP_END'].values[-1]:}.csv\")\n",
    "\n",
    "        \n",
    "cdf = pd.concat(compdf, axis=0)\n",
    "cdf.index.set_names(['stationid','datetime_start'],inplace=True)\n",
    "#cdf.rename(columns={'level_0':'stationid'},inplace=True)\n",
    "#cdf.to_parquet('../station_data/all_data.parquet')\n",
    "for col in cdf.columns:\n",
    "    cdf.rename(columns={col:col.lower()},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577962db",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_folders = {'US-UTD':'Dugout_Ranch',\n",
    "                'US-UTB':'BSF',\n",
    "                'US-UTJ':'Bluff',\n",
    "                'US-UTW':'Wellington',\n",
    "                'US-UTE':'Escalante',\n",
    "                'US-UTM':'Matheson',\n",
    "                'US-UTP':'Phrag',\n",
    "                'US-CdM':'Cedar_mesa',\n",
    "                'US-UTV':'Desert_View_Myton',\n",
    "                'US-UTN':'Juab'\n",
    "                }\n",
    "\n",
    "\n",
    "compdf = {}\n",
    "\n",
    "am = micromet.AmerifluxDataProcessor()\n",
    "\n",
    "for key, value in site_folders.items():\n",
    "\n",
    "    print(key)\n",
    "    raw_fold = pathlib.Path('G:/Shared drives/UGS_Flux/Data_Downloads/')\n",
    "    raw_data = am.raw_file_compile(raw_fold, value, search_str = \"*Statistics_AmeriFlux*.dat\")\n",
    "    if raw_data is not None:\n",
    "        am_data = micromet.Reformatter(raw_data,\n",
    "                                       config_path=\"../../data/reformatter_vars.yml\", \n",
    "                                       drop_soil=False,\n",
    "                                       data_type='met'\n",
    "                                       )\n",
    "        am_df = am_data.et_data\n",
    "        compdf[key] = am_df\n",
    "\n",
    "        #am_df.to_csv(f\"../../station_data/{key}_HH_{am_df['TIMESTAMP_START'].values[0]:}_{am_df['TIMESTAMP_END'].values[-1]:}.csv\")\n",
    "\n",
    "        \n",
    "ddf = pd.concat(compdf, axis=0)\n",
    "ddf.index.set_names(['stationid','datetime_start'],inplace=True)\n",
    "#cdf.rename(columns={'level_0':'stationid'},inplace=True)\n",
    "#cdf.to_parquet('../station_data/all_data.parquet')\n",
    "for col in ddf.columns:\n",
    "    ddf.rename(columns={col:col.lower()},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f082e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soilcols = [col.lower() for col in am_data.MATH_SOILS_V2]\n",
    "soildfs = pd.merge(ddf,cdf[soilcols],how='left',on=['stationid','datetime_start'],suffixes=(None,'_eddy'))\n",
    "soildfs\n",
    "\n",
    "for col in cdf.columns:\n",
    "    if col in soilcols:\n",
    "        cdf.drop(columns=col,inplace=True)  # drop the soil columns from the main dataframe\n",
    "\n",
    "cdf.to_parquet('../../station_data/all_eddy_data.parquet')\n",
    "\n",
    "soildfs.to_parquet('../../station_data/all_soil_data.parquet')\n",
    "\n",
    "ddf.to_parquet('../../station_data/all_met_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f9e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.read_parquet('../../station_data/all_eddy_data.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cc9ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "soildfs = pd.read_parquet('../../station_data/all_soil_data.parquet')\n",
    "utd_soilt = soildfs.loc['US-UTD'][['ts_3_1_1','ts_3_2_1','ts_3_3_1']].replace(-9999,np.nan)\n",
    "utd_soilt = utd_soilt[utd_soilt.index >= '2024-07-01']#.resample('30T').mean()\n",
    "utd_soilt['ts_3_1_1'].plot()\n",
    "utd_soilt['ts_3_2_1'].shift(-1).plot()\n",
    "utd_soilt['ts_3_3_1'].shift(-5).plot()\n",
    "plt.axvline('2024-07-04 15:00',color='r')\n",
    "#plt.xlim('2024-07-01','2024-07-08')\n",
    "#plt.ylim(10,35)\n",
    "plt.grid(True, which='minor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy.signal import correlate\n",
    "\n",
    "# Function to decompose the seasonal component\n",
    "def extract_seasonal(ts, period):\n",
    "    decomposition = seasonal_decompose(ts, model='additive', period=period)\n",
    "    return decomposition.seasonal\n",
    "\n",
    "# Function to calculate lag between two seasonal series using cross-correlation\n",
    "def calculate_lag(seasonal1, seasonal2):\n",
    "    n = len(seasonal1)\n",
    "    correlation = correlate(seasonal1 - np.mean(seasonal1), seasonal2 - np.mean(seasonal2), mode='full')\n",
    "    lags = np.arange(-n + 1, n)\n",
    "    lag = lags[np.argmax(correlation)]\n",
    "    return lag, correlation, lags\n",
    "\n",
    "ts1 = utd_soilt['ts_3_2_1']\n",
    "ts2 = utd_soilt['ts_3_3_1']\n",
    "#utd_soilt['ts_3_3_1'].shift(-5).plot()\n",
    "\n",
    "\n",
    "# Extract seasonal components\n",
    "seasonal1 = extract_seasonal(ts1, period=48)\n",
    "seasonal2 = extract_seasonal(ts2, period=48)\n",
    "\n",
    "# Calculate lag\n",
    "lag, correlation, lags = calculate_lag(seasonal1.dropna(), seasonal2.dropna())\n",
    "\n",
    "# Output\n",
    "print(f\"Calculated lag: {lag/2} hours\")\n",
    "\n",
    "# Plot seasonal components and correlation\n",
    "fig, ax = plt.subplots(3, 1, figsize=(10, 8))\n",
    "\n",
    "seasonal1.plot(ax=ax[0], label='Seasonal Component 1')\n",
    "seasonal2.plot(ax=ax[0], label='Seasonal Component 2')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Seasonal Components')\n",
    "ax[0].set_xlim(pd.to_datetime('2024-07-01'),pd.to_datetime('2024-07-08'))\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(lags, correlation)\n",
    "ax[1].set_title('Cross-Correlation')\n",
    "ax[1].set_xlabel('Lag (hours)')\n",
    "ax[1].set_ylabel('Correlation')\n",
    "ax[1].set_xlim(-10, 10)\n",
    "ax[1].grid(True)\n",
    "\n",
    "ax[2].plot(seasonal1.index, seasonal1, label='Series 1')\n",
    "ax[2].plot(seasonal2.index + pd.Timedelta(hours=lag/2), seasonal2, label='Series 2 (Shifted)')\n",
    "ax[2].legend()\n",
    "ax[2].set_title(f'Series alignment (Lag: {lag/2} hours)')\n",
    "ax[2].set_xlim(pd.to_datetime('2024-07-01'),pd.to_datetime('2024-07-08'))\n",
    "ax[2].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04ee994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc536c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.read_parquet('../../station_data/all_eddy_data.parquet')\n",
    "ddf = pd.read_parquet('../../station_data/all_met_data.parquet')\n",
    "\n",
    "for col in cdf.columns:\n",
    "    if col in ddf.columns:\n",
    "        print(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head(10).to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549094d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = ddf.loc['US-UTD','t_si111_body'].replace(-9999,np.nan)\n",
    "series.plot()\n",
    "series.diff().plot()\n",
    "new_series = series[series.diff()<2].diff().cumsum()\n",
    "new_series.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddcebdfd6a7b51c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T02:58:26.322749Z",
     "start_time": "2024-12-28T02:58:26.266874Z"
    }
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "config.read('../../secrets/config.ini')\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import urllib.parse\n",
    "host = config['DEFAULT']['ip']\n",
    "pw = config['DEFAULT']['pw']\n",
    "user = config['DEFAULT']['login']\n",
    "\n",
    "encoded_password = urllib.parse.quote_plus(pw)\n",
    "\n",
    "def postconn_et(encoded_password, host='localhost',user='postgres',port='5432',db='groundwater', schema = 'groundwater'):\n",
    "    connection_text = \"postgresql+psycopg2://{:}:{:}@{:}:{:}/{:}?gssencmode=disable\".format(user,encoded_password,host,port,db)\n",
    "    return create_engine(connection_text, connect_args={'options': '-csearch_path={}'.format(schema)})\n",
    "\n",
    "\n",
    "engine = postconn_et(encoded_password, host=host, user=user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ce788442a9680",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.to_sql(name = 'amfluxeddy',\n",
    "           schema='groundwater',\n",
    "           con=engine,\n",
    "           if_exists='replace',\n",
    "           chunksize=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5819ddd94230e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "soildfs.to_sql(name = 'amfluxmet',\n",
    "           schema='groundwater',\n",
    "           con=engine,\n",
    "           if_exists='replace',\n",
    "           chunksize=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
