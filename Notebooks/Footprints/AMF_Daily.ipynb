{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-18T00:42:31.469817Z",
     "start_time": "2025-02-18T00:42:29.678746Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "This script was developed to parallel process preformatted time series of input data needed for\n",
    "the Kljun et. al 2d flux footprint prediction code and ultimately create daily-ETo-weighted\n",
    "footprint georeferenced footprint rasters.\n",
    "\n",
    "Checks are performed on the input data to handle data quality issues. The weighting method\n",
    "uses normalized hourly proportions of ASCE ETo computed from NLDAS v2 data for the closest cell.\n",
    "NLDAS data is automatically downloaded using OpenDAP given Earthdata login info. Only days with\n",
    "5 or more hours of data (only from hours between 6:00AM to 8:00 PM) must exist in a day.\n",
    "Checks are performed to ensure final weighting procedure was successful at different steps of\n",
    "the process.\n",
    "\n",
    "This script is not intended to be used by others but to document a workflow that was employed for\n",
    "scientific purposes.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import refet\n",
    "import pyproj as proj\n",
    "import xarray\n",
    "import requests\n",
    "import multiprocessing as mp\n",
    "__author__='John Volk'"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T00:42:36.783543Z",
     "start_time": "2025-02-18T00:42:31.878311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#sys.path.append(\"//\")\n",
    "sys.path.append(\"../../../Micromet\")\n",
    "import micromet\n",
    "from micromet.volk import ffp_climatology as ffp\n",
    "from micromet import AmerifluxDataProcessor"
   ],
   "id": "ab00ddb84f471a24",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T23:02:01.251529Z",
     "start_time": "2025-02-14T23:01:59.329118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read metadata that has each sites' elevation used in ETr/ETo calcs\n",
    "AMF_meta_path = Path('path/to/site_metadata')\n",
    "AMF_meta = pd.read_csv(AMF_meta_path, index_col='SITE_ID')"
   ],
   "id": "d2238d048f9fbd3f",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path\\\\to\\\\site_metadata'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# read metadata that has each sites' elevation used in ETr/ETo calcs\u001B[39;00m\n\u001B[0;32m      2\u001B[0m AMF_meta_path \u001B[38;5;241m=\u001B[39m Path(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpath/to/site_metadata\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m AMF_meta \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mAMF_meta_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mSITE_ID\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\py313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1014\u001B[0m     dialect,\n\u001B[0;32m   1015\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1023\u001B[0m )\n\u001B[0;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\py313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\.conda\\envs\\py313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\py313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1887\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1889\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\.conda\\envs\\py313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'path\\\\to\\\\site_metadata'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# specify path with input CSV files for each station with\n",
    "# input time series of needed data, e.g. zm, u_star, L,...\n",
    "in_dir = Path('dir/with/input')\n",
    "hourly_files = list(in_dir.glob('*.csv'))"
   ],
   "id": "c809498dcc2d640d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5a80ba7f831a3a16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T03:44:17.152189Z",
     "start_time": "2025-02-15T03:44:17.144508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import configparser\n",
    "# load initial flux data\n",
    "station = 'US-UTW'\n",
    "config_path = f'../../station_config/{station}.ini'\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_path)\n",
    "\n",
    "spath = \"../../secrets/config.ini\"\n",
    "sconfig = configparser.ConfigParser()\n",
    "sconfig.read(spath)\n",
    "\n",
    "ed_pass = sconfig['NLDAS']['pw']\n",
    "ed_user = sconfig['NLDAS']['user']"
   ],
   "id": "9374b88929675d4c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T03:45:02.808722900Z",
     "start_time": "2025-02-15T03:44:22.830530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "\n",
    "#nldas_out_dir = Path('C:/Users/paulinkenbrandt/Documents/GitHub/MicroMet/Notebooks/Footprints')\n",
    "\n",
    "\n",
    "for date in pd.date_range(start='2022-01-01', end='2025-12-31',freq='h'):\n",
    "    hour = date.hour\n",
    "    if 5 <= hour <= 21:\n",
    "        micromet.download_nldas(date,\n",
    "                           hour,\n",
    "                           ed_user,\n",
    "                           ed_pass,)"
   ],
   "id": "cd9d77b60b11d20f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def runner(path, ed_user, ed_pass):\n",
    "    \"\"\"\n",
    "    Given path to time series of site hourly (or finer) input data,\n",
    "    compute daily ETo weighted footprint rasters.\n",
    "\n",
    "    Requires NASA Earthdata username and password to download NLDAS-v2\n",
    "    primary forcing at point locations for estimated ASCE short ref. ET.\n",
    "    \"\"\"\n",
    "\n",
    "    df, latitude, longitude = micromet.read_compiled_input(path)\n",
    "    station = path.stem\n",
    "    elevation = AMF_meta.loc[station, 'station_elevation']\n",
    "\n",
    "    station_coord = (longitude, latitude)\n",
    "    # get EPSG code from lat,long, convert to UTM\n",
    "    EPSG=32700-np.round((45+latitude)/90.0)*100+np.round((183+longitude)/6.0)\n",
    "    EPSG = int(EPSG)\n",
    "    in_proj = proj.Proj(init='EPSG:4326')\n",
    "    out_proj = proj.Proj(init='EPSG:{}'.format(EPSG))\n",
    "    (station_x,station_y) = proj.transform(in_proj,out_proj,*station_coord)\n",
    "    print('original coordinates:',station_x,station_y)\n",
    "    # move coord to snap centroid to 30m grid, minimal distortion\n",
    "    station_x, station_y = micromet.snap_centroid(station_x, station_y)\n",
    "\n",
    "    #Other model parameters\n",
    "    h_s = 2000. #Height of atmos. boundary layer [m] - assumed\n",
    "    dx = 30. #Model resolution [m]\n",
    "    origin_d = 300. #Model bounds distance from origin [m]\n",
    "    #modify if needed\n",
    "    start_hr = 6 # hours from 1 to 24\n",
    "    end_hr = 18\n",
    "    hours_zero_indexed = np.arange(start_hr-1,end_hr)\n",
    "    hours_one_indexed = np.arange(start_hr,end_hr+1)\n",
    "\n",
    "    n_hrs = len(hours_zero_indexed)\n",
    "\n",
    "    nldas_out_dir = Path('NLDAS_data')\n",
    "    if not nldas_out_dir.is_dir():\n",
    "        nldas_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out_dir = Path('All_output')/'AMF'/f'{station}'\n",
    "\n",
    "    if not out_dir.is_dir():\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    #Loop through each day in the dataframe\n",
    "    for date in df.index.date:\n",
    "        #Subset dataframe to only values in day of year\n",
    "        print(f'Date: {date}')\n",
    "        temp_df = df[df.index.date == date]\n",
    "        temp_df=temp_df.between_time(f'{start_hr:02}:00', f'{end_hr:02}:00')\n",
    "        # check on n hours per day\n",
    "        if len(temp_df) < 5:\n",
    "            print(f'Less than 5 hours of data on {date}, skipping.')\n",
    "            continue\n",
    "\n",
    "        new_dat = None\n",
    "\n",
    "        out_f = out_dir/ f'{date}.tif'\n",
    "\n",
    "        final_outf = out_dir/f'{date.year}-{date.month:02}-{date.day:02}_weighted.tif'\n",
    "        if final_outf.is_file():\n",
    "            print(f'final daily weighted footprint already wrote to: {final_outf}\\nskipping.')\n",
    "            continue # do not overwrite date/site raster\n",
    "\n",
    "        # make hourly band raster for the day\n",
    "        for indx, hour in enumerate(hours_zero_indexed):\n",
    "\n",
    "            band = indx + 1\n",
    "            print(f'Hour: {hour}')\n",
    "\n",
    "            try:\n",
    "                temp_line = temp_df.loc[temp_df.index.hour == hour,:]\n",
    "                if temp_line.empty:\n",
    "                    print(f'Missing all data for {date,hour} skipping')\n",
    "                    continue\n",
    "                zm = temp_line.zm.values - temp_line.d.values\n",
    "                z0 = temp_line.z0.values if 'z0' in temp_line.columns else None\n",
    "                u_mean = temp_line.u_mean.values if 'u_mean' in temp_line.columns else None\n",
    "                if u_mean is not None: z0 = None\n",
    "\n",
    "                #Calculate footprint\n",
    "                temp_ffp = ffp(domain=[-origin_d,origin_d,-origin_d,origin_d],dx=dx,dy=dx,\n",
    "                                        zm=zm, h=h_s, rs=None, z0=z0,\n",
    "                                        ol=temp_line['L'].values,sigmav=temp_line['sigma_v'].values,\n",
    "                                        ustar=temp_line['u_star'].values, umean=u_mean,\n",
    "                                        wind_dir=temp_line['wind_dir'].values,\n",
    "                                        crop=0,fig=0,verbosity=0)\n",
    "                f_2d = np.array(temp_ffp['fclim_2d'])\n",
    "                x_2d = np.array(temp_ffp['x_2d']) + station_x\n",
    "                y_2d = np.array(temp_ffp['y_2d']) + station_y\n",
    "                f_2d = f_2d*dx**2\n",
    "\n",
    "                #Calculate affine transform for given x_2d and y_2d\n",
    "                affine_transform = micromet.find_transform(x_2d,y_2d)\n",
    "\n",
    "                #Create data file if not already created\n",
    "                if new_dat is None:\n",
    "                    #print(f_2d.shape)\n",
    "                    new_dat = rasterio.open(\n",
    "                        out_f,'w',driver='GTiff',dtype=rasterio.float64,\n",
    "                        count=n_hrs,height=f_2d.shape[0],width=f_2d.shape[1],\n",
    "                        transform=affine_transform, crs=out_proj.srs,\n",
    "                        nodata=0.00000000e+000\n",
    "                    )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'Hour {hour} footprint failed, band {band} not written.')\n",
    "\n",
    "                temp_ffp = None\n",
    "\n",
    "                continue\n",
    "\n",
    "            #Mask out points that are below a % threshold (defaults to 90%)\n",
    "            f_2d = micromet.mask_fp_cutoff(f_2d)\n",
    "\n",
    "            #Write the new band\n",
    "            new_dat.write(f_2d, indx+1)\n",
    "\n",
    "            #Update tags with metadata\n",
    "            tag_dict = {'hour':f'{hour*100:04}',\n",
    "                        'wind_dir':temp_line['wind_dir'].values,\n",
    "                        'total_footprint':np.nansum(f_2d)}\n",
    "\n",
    "            new_dat.update_tags(indx+1,**tag_dict)\n",
    "\n",
    "        #Close dataset if it exists\n",
    "        try:\n",
    "            new_dat.close()\n",
    "        except:\n",
    "            print(f'ERROR: could not write footprint for site: {station}:\\nto: {out_f}')\n",
    "            continue # skip to next day...\n",
    "\n",
    "        # for NLDAS data from pymetric\n",
    "        for hour in hours_zero_indexed:\n",
    "            micromet.download_nldas(date, hour, ed_user, ed_pass)\n",
    "            nldas_df = micromet.calc_nldas_refet(date, hour, nldas_out_dir, latitude, longitude, elevation, zm)\n",
    "\n",
    "        # do hourly weighting - do not necessarily need to do this all in the same loop\n",
    "        src = rasterio.open(out_f)\n",
    "        # hourly fetch scalar sums\n",
    "        global_sum = np.zeros(shape=(n_hrs))\n",
    "        for hour in range(1,n_hrs+1):\n",
    "            arr = src.read(hour)\n",
    "            global_sum[hour-1] = arr.sum()\n",
    "        # normalized fetch rasters\n",
    "        normed_fetch_rasters = []\n",
    "        for hour in range(1,n_hrs+1):\n",
    "            arr = src.read(hour)\n",
    "            tmp = arr / global_sum[hour-1]\n",
    "            if np.isnan(tmp).all():\n",
    "                tmp = np.zeros_like(tmp)\n",
    "            normed_fetch_rasters.append(tmp)\n",
    "\n",
    "        out_dir = Path('All_output')/'AMF'/f'{station}'\n",
    "\n",
    "        if not out_dir.is_dir():\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # this one is saved under the site_ID subdir\n",
    "        nldas_ts_outf = out_dir/ f'nldas_ETr.csv'\n",
    "        # get NLDAS ts calc fraction of daily ETo\n",
    "        nldas_df = pd.read_csv(nldas_ts_outf, index_col='date', parse_dates=True).sort_index()\n",
    "        ETo = nldas_df.loc[nldas_df.index.date == date, 'ETo']\n",
    "        min_max_normed_ETo = (ETo-min(ETo))/(max(ETo)-min(ETo)) # deal with negative ETo value proportions\n",
    "        # take out hours where footprint does not exist\n",
    "        i = 0\n",
    "        for e, s in zip(min_max_normed_ETo.values, global_sum):\n",
    "            if s == 0:\n",
    "                min_max_normed_ETo.iloc[i] = 0\n",
    "            i+=1\n",
    "        # after removing hours now calculate hourly proportions\n",
    "        nldas_df.loc[nldas_df.index.date == date, 'ETo_hr_props'] = min_max_normed_ETo / min_max_normed_ETo.sum()\n",
    "        # weight normed hourly fetch rasters by hourly ETo proportions\n",
    "        for i,hour in enumerate(hours_zero_indexed): # everything here is hours 0-23\n",
    "            normed_fetch_rasters[i] =\\\n",
    "                normed_fetch_rasters[i]*nldas_df.loc[\n",
    "                    (nldas_df.index.date == date) & (nldas_df.index.hour == hour), 'ETo_hr_props'\n",
    "            ].values[0]\n",
    "        # save hourly proportions to time series file\n",
    "        nldas_df.round(4).to_csv(nldas_ts_outf)\n",
    "\n",
    "        # Last calculation, sum the weighted hourly rasters to a single daily fetch raster\n",
    "        final_footprint = sum(normed_fetch_rasters)\n",
    "        assert np.isclose(final_footprint.sum(), 1), f'check 1 failed! {final_footprint.sum()}\\n{temp_line}'\n",
    "        # next check\n",
    "        for hour, raster in enumerate(normed_fetch_rasters):\n",
    "            assert np.isclose(\n",
    "                nldas_df.loc[\n",
    "                    (nldas_df.index.date == date) & (nldas_df.index.hour == hour+start_hr-1), 'ETo_hr_props'\n",
    "                ].values[0], raster.sum()\n",
    "            ), f'check 2 failed for hour {hour+start_hr-1}!!!'\n",
    "\n",
    "        # finally, write daily corrected raster with UTM zone reference\n",
    "        corr_raster_path = final_outf\n",
    "        out_raster = rasterio.open(\n",
    "            corr_raster_path,'w',driver='GTiff',dtype=rasterio.float64,\n",
    "            count=1,height=final_footprint.shape[0],width=final_footprint.shape[1],\n",
    "            transform=src.transform, crs=out_proj.srs, nodata=0.00000000e+000\n",
    "        )\n",
    "        out_raster.write(final_footprint,1)\n",
    "        out_raster.close()\n"
   ],
   "id": "c3387bff7a0a7ac0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pool = mp.Pool(processes=8)\n",
    "pool.map(runner,hourly_files)"
   ],
   "id": "1e66a0ffd01f493e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
