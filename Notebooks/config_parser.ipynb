{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40cf2ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from configparser import ConfigParser\n",
    "\n",
    "import polars as pl\n",
    "import json\n",
    "import re\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa1db174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../micromet\")\n",
    "import micromet\n",
    "from micromet import AmerifluxDataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06e5cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_polars_df_iqr(df, \n",
    "                         column_name, \n",
    "                         lower_quant = 0.05, \n",
    "                         upper_quant= 0.95, \n",
    "                         multiplier=1., \n",
    "                         filter_rows=True,\n",
    "                         replace_value=None \n",
    "                         ):\n",
    "    \"\"\"\n",
    "    Filters a Polars DataFrame based on the IQR method for outlier detection.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pl.DataFrame): The Polars DataFrame to filter.\n",
    "    column_name (str): The name of the column to apply the IQR filter on.\n",
    "    \n",
    "    Returns:\n",
    "    pl.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "    q1 = df.select(pl.col(column_name).quantile(lower_quant)).item()\n",
    "    q3 = df.select(pl.col(column_name).quantile(upper_quant)).item()\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    lower_bound = q1 - multiplier * iqr\n",
    "    upper_bound = q3 + multiplier * iqr\n",
    "\n",
    "    if filter_rows:\n",
    "        return df.filter(\n",
    "            (pl.col(column_name) >= lower_bound) & (pl.col(column_name) <= upper_bound)\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        return df.with_columns(\n",
    "            pl.when(pl.col(column_name) < lower_bound)\n",
    "            .then(replace_value)\n",
    "            .when(pl.col(column_name) > upper_bound)\n",
    "            .then(replace_value)\n",
    "            .otherwise(pl.col(column_name)).alias(column_name)\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b01b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    TIMESTAMP                     parsed\n",
      "0         2023-10-27 15:59:55 2023-10-27 15:59:55.000000\n",
      "1        2023-10-27 15:59:55. 2023-10-27 15:59:55.000000\n",
      "2  2023-10-27 15:59:55.123456 2023-10-27 15:59:55.123456\n",
      "3         2023-10-27 00:00:00 2023-10-27 00:00:00.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def replace_negative_values(df, column_name, filter_column=None, threshold=0.85, replace_value=0):\n",
    "    \"\"\"\n",
    "    Replace negative values in a specified column of a DataFrame with 0.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "        column_name (str): The name of the column to check for negative values.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with negative values replaced by 0.\n",
    "    \"\"\"\n",
    "    if filter_column:\n",
    "        filter_column = filter_column\n",
    "    else:\n",
    "        filter_column = column_name\n",
    "\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(filter_column) < threshold)\n",
    "        .then(replace_value)\n",
    "        .otherwise(pl.col(column_name))\n",
    "        .alias(column_name)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def parse_datetimes_with_decimal_seconds(datetime_series):\n",
    "    \"\"\"\n",
    "    Parse datetime strings that may have optional or missing fractional seconds,\n",
    "    and normalize them so all include a decimal part.\n",
    "\n",
    "    Parameters:\n",
    "        datetime_series (pd.Series or list-like): Datetime strings in format\n",
    "        'YYYY-MM-DD HH:MM:SS', 'YYYY-MM-DD HH:MM:SS.', or 'YYYY-MM-DD HH:MM:SS.ssssss'.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Parsed datetime values with consistent fractional seconds.\n",
    "    \"\"\"\n",
    "    def normalize_timestamp(ts):\n",
    "        ts = ts.strip()\n",
    "        # Remove trailing dot if no digits follow\n",
    "        ts = re.sub(r'(?<=\\d{2}:\\d{2}:\\d{2})\\.(?!\\d)', '', ts)\n",
    "        # If it ends right after the seconds, append \".0\"\n",
    "        if re.match(r'^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}$', ts):\n",
    "            ts += '.0'\n",
    "        return ts\n",
    "\n",
    "    cleaned = datetime_series.map(normalize_timestamp)\n",
    "    return pd.to_datetime(cleaned, format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "example = [\n",
    "    \"2023-10-27 15:59:55\",\n",
    "    \"2023-10-27 15:59:55.\",\n",
    "    \"2023-10-27 15:59:55.123456\",\n",
    "    \"2023-10-27 00:00:00\"\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({'TIMESTAMP': example})\n",
    "df['parsed'] = parse_datetimes_with_decimal_seconds(df['TIMESTAMP'])\n",
    "print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f137bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from micromet import convert_file\n",
    "import pathlib\n",
    "\n",
    "input_file = pathlib.Path(\"G:/Shared drives/UGS_Flux/Data_Downloads/Dugout_Ranch/e20240415/21314_Time_Series_100.dat\")\n",
    "output_file = \"G:/Shared drives/UGS_Flux/Data_Downloads/Dugout_Ranch/e20240415/21314_Time_Series_100.asc\"\n",
    "output_format = \"toa5\"\n",
    "format_options = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e556e7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Check if input file exists\n",
    "if not input_file.exists():\n",
    "    print(f\"Error: Input file '{input_file}' does not exist.\")\n",
    "\n",
    "\n",
    "convert_file(input_file, output_file, output_format, format_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "475cdf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECORD Int64\n",
      "Ux Float64\n",
      "Uy Float64\n",
      "Uz Float64\n",
      "T_SONIC Float64\n",
      "diag_sonic Int64\n",
      "CO2_density Float64\n",
      "CO2_density_fast_tmpr Float64\n",
      "H2O_density Float64\n",
      "diag_irga Int64\n",
      "T_SONIC_corr Float64\n",
      "TA_1_1_1 Float64\n",
      "PA Float64\n",
      "CO2_sig_strgth Float64\n",
      "H2O_sig_strgth Float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Step 1 & 2: Read lines manually to get column names and units\n",
    "with open(output_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    column_names = [col.strip().strip('\"') for col in lines[1].split(\",\")]\n",
    "    units = [unit.strip().strip('\"') for unit in lines[2].split(\",\")]\n",
    "\n",
    "# Step 3: Create a units dictionary\n",
    "unit_map = dict(zip(column_names, units))\n",
    "\n",
    "# Step 4: Read data using polars, skipping 4 lines and setting column names\n",
    "df = pl.read_csv(\n",
    "    output_file,\n",
    "    skip_rows=4,\n",
    "    has_header=False,  # because we are setting our own column names\n",
    "    new_columns=column_names,\n",
    "    quote_char='\"',\n",
    "    try_parse_dates=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for col in df.columns:\n",
    "    if col != \"TIMESTAMP\":\n",
    "        print(col, df[col].dtype)\n",
    "        df = filter_polars_df_iqr(df, col, filter_rows=False)\n",
    "\n",
    "df = replace_negative_values(df, 'H2O_density', filter_column='H2O_sig_strgth', threshold=0.85, replace_value=0)\n",
    "#df[\"TIMESTAMP\"] = parse_polars_datetimes_with_decimal_seconds(df['TIMESTAMP'])\n",
    "# Step 1: Convert Polars â†’ Pandas\n",
    "df_pd = df.to_pandas()\n",
    "df_pd[\"TIMESTAMP\"] = parse_datetimes_with_decimal_seconds(df_pd[\"TIMESTAMP\"])\n",
    "\n",
    "\n",
    "\n",
    "# Save the units dictionary to a JSON file\n",
    "with open(\"units.json\", \"w\") as f:\n",
    "    json.dump(unit_map, f, indent=2)\n",
    "\n",
    "df_pd = df_pd.set_index([\"TIMESTAMP\"])\n",
    "df_resamp = df_pd.resample(\"100L\").asfreq()\n",
    "\n",
    "# Step 2: Impute with IterativeImputer\n",
    "imputer = IterativeImputer(random_state=0)\n",
    "df_imputed_pd = pd.DataFrame(imputer.fit_transform(df_resamp), \n",
    "                             columns=df_resamp.columns, \n",
    "                             index=df_resamp.index)\n",
    "\n",
    "\n",
    "\n",
    "df_imputed = pl.from_pandas(df_imputed_pd)\n",
    "\n",
    "df_imputed = replace_negative_values(df_imputed, 'H2O_density')\n",
    "\n",
    "datalogger = str(input_file.stem).split(\"_\")[0]\n",
    "fileno = str(input_file.stem).split(\"_\")[-1] \n",
    "first_date = df_imputed_pd.first_valid_index()\n",
    "minutes = round((df_imputed_pd.last_valid_index() - df_imputed_pd.first_valid_index()).seconds/60,0) \n",
    "meas_freq = len(df_imputed_pd)/(minutes*60) # Hz\n",
    "first_date\n",
    "\n",
    "df_imputed.write_csv(f\"{datalogger}_imputed_{fileno}_{first_date:%Y_%m_%d_%H%M}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "179ee5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TIMESTAMP': 'TS',\n",
       " 'RECORD': 'RN',\n",
       " 'Ux': 'm s-1',\n",
       " 'Uy': 'm s-1',\n",
       " 'Uz': 'm s-1',\n",
       " 'T_SONIC': 'deg C',\n",
       " 'diag_sonic': 'adimensional',\n",
       " 'CO2_density': 'mg m-3',\n",
       " 'CO2_density_fast_tmpr': '',\n",
       " 'H2O_density': 'g m-3',\n",
       " 'diag_irga': 'adimensional',\n",
       " 'T_SONIC_corr': 'deg C',\n",
       " 'TA_1_1_1': 'deg C',\n",
       " 'PA': 'kPa',\n",
       " 'CO2_sig_strgth': 'fraction',\n",
       " 'H2O_sig_strgth': 'fraction'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unit_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eec547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df_imputed_pd['H2O_density'].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78d43b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_line(filename, line_to_prepend):\n",
    "    with open(filename, 'r+', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        f.seek(0, 0)\n",
    "        f.write(line_to_prepend.rstrip('\\r\\n') + '\\n' + content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95bbe577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "config_path = f'../station_config/datafile.metadata'\n",
    "# instantiate\n",
    "config = ConfigParser()\n",
    "\n",
    "# parse existing file\n",
    "config.read(config_path)\n",
    "\n",
    "fpath = pathlib.Path(r\"C:\\Users\\paulinkenbrandt\\Downloads\\UGS Flux SDI12 Addresses Pakbus Metadata and Connection - Site_metadata.csv\")\n",
    "df = pd.read_csv(fpath)\n",
    "df.dropna(subset=['stationid'], inplace=True)\n",
    "df\n",
    "\n",
    "for stat in df.index:\n",
    "    site_id = df.loc[stat, 'stationid']\n",
    "    name = df.loc[stat, 'parameter']\n",
    "    latitude = df.loc[stat, 'latitude']\n",
    "    longitude = df.loc[stat, 'longitude']\n",
    "    altitude = df.loc[stat, 'altitude']\n",
    "    canopy_height = df.loc[stat, 'height_canopy']\n",
    "\n",
    "    config.set('Station', 'station_name', f\"{name}\")\n",
    "    config.set('Station', 'station_id', f\"{site_id}\")\n",
    "    config.set('Site','site_name', f\"{name}\")\n",
    "    config.set('Site','site_id', f\"{site_id}\")\n",
    "    config.set('Site','altitude', f\"{altitude}\")\n",
    "    config.set('Site','latitude', f\"{latitude}\")\n",
    "    config.set('Site','longitude', f\"{longitude}\")    \n",
    "    config.set('Site','canopy_height', f\"{canopy_height}\")\n",
    "    config.set('Timing','acquisition_frequency', f\"{int(meas_freq):0d}\")\n",
    "    config.set('Timing','file_duration', f\"{int(minutes):0d}\")\n",
    "\n",
    "    config.set('Project','title', f\"{name}_{int(minutes):2d}min_process\")\n",
    "    config.set('Project','id', f\"{site_id.split(\"-\")[-1]}\")\n",
    "\n",
    "    with open(f'../station_config/edpro_{site_id.split(\"-\")[-1]}.metadata', 'w') as configfile:\n",
    "        config.write(configfile)\n",
    "    prepend_line(f'../station_config/edpro_{site_id.split(\"-\")[-1]}.metadata', \";GHG_METADATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8ff0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d59648",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
